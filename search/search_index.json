{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Controllers for Kubernetes \u00b6 AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. This is a new open source project built with \u2764\ufe0f by AWS and available as a Developer Preview . We encourage you to try it out, provide feedback and contribute to development. Important: Because this project is a preview, there may be significant and breaking changes introduced in the future. We encourage you to try and experiment with this project. Please do not adopt it for production use. Background \u00b6 Kubernetes applications often require a number of supporting resources like databases, message queues, and object stores to operate. AWS provides a set of managed services that you can use to provide these resources for your applications, but provisioning and integrating them with Kubernetes was complex and time consuming. ACK lets you define and consume many AWS services and resources directly within a Kubernetes cluster. ACK gives you a unified, operationally seamless way to manage your application and its dependencies. Connecting Kubernetes and AWS APIs \u00b6 ACK is a collection of Kubernetes Custom Resource Definitions (CRDs) and controllers which work together to extend the Kubernetes API and create AWS resources on your cluster\u2019s behalf. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the ACK service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API and configuration language to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS service resources upon which those applications depend. Read more about how ACK works . Getting started \u00b6 Until we've graduated ACK service controllers we ask you to test-drive them. Getting help \u00b6 For help, please consider the following venues (in order): Search open issues File an issue Chat with us on the #provider-aws channel in the Kubernetes Slack community.","title":"Overview"},{"location":"#aws-controllers-for-kubernetes","text":"AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. This is a new open source project built with \u2764\ufe0f by AWS and available as a Developer Preview . We encourage you to try it out, provide feedback and contribute to development. Important: Because this project is a preview, there may be significant and breaking changes introduced in the future. We encourage you to try and experiment with this project. Please do not adopt it for production use.","title":"AWS Controllers for Kubernetes"},{"location":"#background","text":"Kubernetes applications often require a number of supporting resources like databases, message queues, and object stores to operate. AWS provides a set of managed services that you can use to provide these resources for your applications, but provisioning and integrating them with Kubernetes was complex and time consuming. ACK lets you define and consume many AWS services and resources directly within a Kubernetes cluster. ACK gives you a unified, operationally seamless way to manage your application and its dependencies.","title":"Background"},{"location":"#connecting-kubernetes-and-aws-apis","text":"ACK is a collection of Kubernetes Custom Resource Definitions (CRDs) and controllers which work together to extend the Kubernetes API and create AWS resources on your cluster\u2019s behalf. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the ACK service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API and configuration language to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS service resources upon which those applications depend. Read more about how ACK works .","title":"Connecting Kubernetes and AWS APIs"},{"location":"#getting-started","text":"Until we've graduated ACK service controllers we ask you to test-drive them.","title":"Getting started"},{"location":"#getting-help","text":"For help, please consider the following venues (in order): Search open issues File an issue Chat with us on the #provider-aws channel in the Kubernetes Slack community.","title":"Getting help"},{"location":"how-it-works/","text":"How it works \u00b6 The idea behind AWS Controllers for Kubernetes (ACK) is to enable Kubernetes users to describe the desired state of AWS resources using the Kubernetes API and configuration language. In order to make this happen, let's take a look under the covers and walk through how different components in the system interact. In the diagram above, Alice is our Kubernetes user. Her application depends on the existence of an Amazon S3 Bucket named my-bucket . Instead of creating the S3 Bucket via the AWS web console, Alice wants to only use the Kubernetes API. After all, Alice uses the Kubernetes API to describe all her application resources -- a Deployment , a Service , an Ingress , etc. She'd like to use the Kubernetes API to describe all the resources her application requires, including this S3 Bucket. So, Alice issues a call to kubectl apply , passing in a file that describes a Kubernetes custom resource describing her S3 Bucket. kubectl apply passes this file, called a Manifest , to the Kubernetes API server running in the Kubernetes controller node. (1) The Kubernetes API server receives the Manifest describing the S3 Bucket and determines if Alice has permissions to create a custom resource (CR) of Kind s3.services.k8s.aws/Bucket , and that the custom resource is properly formatted (2) . If Alice is authorized and the CR is valid, the Kubernetes API server writes (3) the CR to its etcd data store and then responds back (4) to Alice that the CR has been created. At this point, the ACK service controller for S3, which is running on a Kubernetes worker node within the context of a normal Kubernetes Pod , is notified (5) that a new CR of Kind s3.services.k8s.aws/Bucket has been created. The ACK service controller for S3 then communicates (6) with the AWS S3 API, calling the S3 CreateBucket API call to create the bucket in AWS. After communicating with the S3 API, the ACK service controller then calls the Kubernetes API server to update (7) the CR's Status with information it received from S3.","title":"How it works"},{"location":"how-it-works/#how-it-works","text":"The idea behind AWS Controllers for Kubernetes (ACK) is to enable Kubernetes users to describe the desired state of AWS resources using the Kubernetes API and configuration language. In order to make this happen, let's take a look under the covers and walk through how different components in the system interact. In the diagram above, Alice is our Kubernetes user. Her application depends on the existence of an Amazon S3 Bucket named my-bucket . Instead of creating the S3 Bucket via the AWS web console, Alice wants to only use the Kubernetes API. After all, Alice uses the Kubernetes API to describe all her application resources -- a Deployment , a Service , an Ingress , etc. She'd like to use the Kubernetes API to describe all the resources her application requires, including this S3 Bucket. So, Alice issues a call to kubectl apply , passing in a file that describes a Kubernetes custom resource describing her S3 Bucket. kubectl apply passes this file, called a Manifest , to the Kubernetes API server running in the Kubernetes controller node. (1) The Kubernetes API server receives the Manifest describing the S3 Bucket and determines if Alice has permissions to create a custom resource (CR) of Kind s3.services.k8s.aws/Bucket , and that the custom resource is properly formatted (2) . If Alice is authorized and the CR is valid, the Kubernetes API server writes (3) the CR to its etcd data store and then responds back (4) to Alice that the CR has been created. At this point, the ACK service controller for S3, which is running on a Kubernetes worker node within the context of a normal Kubernetes Pod , is notified (5) that a new CR of Kind s3.services.k8s.aws/Bucket has been created. The ACK service controller for S3 then communicates (6) with the AWS S3 API, calling the S3 CreateBucket API call to create the bucket in AWS. After communicating with the S3 API, the ACK service controller then calls the Kubernetes API server to update (7) the CR's Status with information it received from S3.","title":"How it works"},{"location":"releases/","text":"Releases, Versioning and Maintenance Phases \u00b6 Service controllers are built in separate source code repositories. Below, you will find a description of the Project Stages that a service controller repository goes through on its way to being released. A controller that has reached the RELEASED project stage will have a set of release artifacts, including binary Docker images for the controller and a Helm Chart that installs the controller into a target Kubernetes cluster. Read more below about our Releases and Versioning policy . Finally, we have a set of documented Maintenance Phases that clearly outline our support stance for service controllers that have been released. Project Stages \u00b6 The controller's \"project stage\" describes how far along the controller is towards being released: PROPOSED -> PLANNED -> IN PROGRESS -> RELEASED PROPOSED \u00b6 The PROPOSED stage indicates that there is expressed interest in supporting an AWS service in ACK. At this stage, there will be a Github Issue and/or a Github Project for tracking the creation of the ACK service controller for the service. The GitHub Issue WILL NOT be associated with a GitHub Milestone. PLANNED \u00b6 The PLANNED stage indicates that we plan to make a controller for this service available in ACK. At this stage, there WILL BE a GitHub Milestone that tracks progress towards the release of the controller. IN PROGRESS \u00b6 The IN PROGRESS stage indicates that the ACK service controller for the AWS service is actively being built in preparation for a release of that ACK service controller. In the IN PROGRESS stage we identify those AWS service API resources that will be supported by the controller and generate the code that manages the lifecycle of these resources. !!! note \"what do we mean by 'AWS service API resources'? An AWS service API resource is a top-level object that can be created by a particular AWS service API. For example, an SNS Topic or an S3 Bucket. Some service APIs have multiple top-level resources; SNS, for instance, has Topic, PlatformApplication and PlatformEndpoint top-level resources that may be created. RELEASED \u00b6 The RELEASED project stage indicates that the ACK service controller source repository has had a Semantic Versioning Git tag applied and that both a Docker image and Helm Chart have been built and published to the ECR Public repositories for ACK. Once a service controller reaches the RELEASED project stage, that does not mean that there can never be any changes or additions to the Custom Resource Definitions (CRDs) or public interfaces exposed by that service controller. The RELEASED project stage is simply an indication that there is at least one SemVer-tagged binary release of the controller. Consumers should look to the SemVer release tag as an indication of whether code included in that release introduces new breaking (major version increment) or non-breaking features (minor version increment) or simply bug fixes (patch version increment). Consumers should see release notes for a release tag for a full description of changes included in that release. Releases and Versioning \u00b6 Important ACK does not have a single release status or version. Different components within the ACK project have different release cadences, versions and statuses. Please read the information below before installing any ACK component. Service controllers in ACK use Semantic Versioning to indicate whether changes included in a particular binary release introduce features or bug fixes and whether or not features break backwards compatibility for public APIs and interfaces. There are two release artifacts produced when an ACK service controller is released: a binary Docker image with the controller and a Helm Chart that installs the controller into a target Kubernetes cluster. Both these artifacts will have tags that correspond to the Semantic Version Git tag applied against the source code repository for the controller. Service controllers may have a Stable Helm Chart that will install a version of the service controller binary that the maintainer team is confident will hold up to production use. Semantic Versioning \u00b6 ACK is a collection of custom Kubernetes controllers, one for each supported AWS API. Each ACK controller is composed of an ACK common runtime and Go code that links the Kubernetes API and the AWS API. Much of this Go code is generated by the ack-generate tool; some of the Go code is hand-crafted. All code components in ACK use Semantic Versioning (SemVer) as a signal to consumers whether public interfaces or APIs have breaking changes. When an ACK component is released , a Git tag containing a SemVer (X.Y.Z) is created on the component's source repository. If the commits to the source repository in between the last Git tag and the commit being tagged have introduced changes that break public-facing APIs or interfaces, the SemVer will have its major version (\"X\") incremented. If the commits introduce functionality that does not break interfaces or APIs, the minor version (\"Y\") will be incremented. If the commits simply fix bugs and do not introduce any features or interface changes, the patch version (\"Z\") will be incremented. Releases of any ACK component that have a zero major release number (e.g. v0.0.2 ) may have breaking changes to the public API or interfaces exposed by that component. This is by design, and per the Semantic Versioning specification : Major version zero (0.y.z) is for initial development. Anything MAY change at any time. The public API SHOULD NOT be considered stable. For ACK components that have a binary distributable -- i.e. a Docker image -- the creation of a new SemVer Git tag on the source code repository will trigger automatic building and publishing of a Docker image with an image tag including the SemVer version. For example, if a Git tag of v1.2.6 was created on the github.com/aws-controllers-k8s/s3-controller repository, a Docker image with a tag s3-v1.2.6 would be published to the aws-controllers-k8s/controller ECR repository. Note Binaries for ACK components are published in our Amazon ECR Public registry . For ACK components that have a Helm Chart distributable -- i.e. an ACK service controller -- the creation of a new SemVer Git tag on the source code repository will trigger automatic building and publishing of a Helm Chart with an artifact tag including the SemVer version. For example, a Git tag of v1.2.6 on the github.com/aws-controllers-k8s/s3-controller repository means a Helm chart with a tag s3-v1.2.6 would be published to the aws-controllers-k8s/chart ECR repository. A Word About Dependencies \u00b6 Each service-specific ACK controller -- e.g. the ElastiCache ACK controller -- depends on a specific version of the ACK common runtime. This dependency is specified in the controller's go.mod file. The ACK code generator that produces Go code for service controllers depends on a specific version of the ACK common runtime. dependency between the code generator and common runtime The ACK code generator depends on the ACK common runtime in a unique way: the Go code that the ACK code generator produces adheres to a specific version of the ACK common runtime. Even though no Go code in the ACK code generator actually imports the ACK common runtime, this dependency exists because the Go code produced by the templates inside the code generator imports the ACK common runtime. In order to make this Go code dependency more strict, we have a test package inside the ACK code generator that imports the ACK common runtime. In this way, we're able to include a version-specific dependency line in the ACK code generator's go.mod file, thereby allowing Go's module infrastructure to pin the dependency between the code generator and the common runtime. Stable Helm Charts \u00b6 Tip We recommend using Helm to install an ACK service controller. Some ACK service controllers will have Helm Charts with a $SERVICE-v$MAJOR_VERSION-stable tag, referred from here out as just a \" stable artifact tag\". There will only be one of these tags for the ACK service controller in a major version series . For example, the stable artifact tag for the ElastiCache ACK service controller's \"v1\" major version series would be elasticache-v1-stable . This stable artifact tag points to a Helm chart that has configuration values that have been tested with a specific SemVer Docker image. Typically these tests are \"soak\" tests and allow the team maintaining that ACK controller's source code to have a high degree of confidence in the controller's long-running operation. Note Please note that not all ACK service controllers will have a Helm chart with a stable artifact tag. Furthermore, there will only ever be a single stable Helm Chart tag per major version series of a controller . This stable Helm Chart tag (an OCI Artifact tag) will point to different Helm Chart packages over time. From time to time, the maintainer team for a service controller may update the configuration values and associated SemVer Docker image tag for the controller binary to point to a newer image. For example, consider the ElastiCache ACK service controller maintainer team has executed a series of long-running tests of the controller image tagged with the elasticache-v1.2.6 SemVer tag. The maintainer team is confident that the controller is stable for production use. In the stable Git branch of the ElastiCache service controller's source repository, the team would update the Helm Chart's Deployment, setting the Deployment.spec.template.spec.containers[0].image to public.ecr.aws/aws-controllers-k8s/controller/elasticache-v1.2.6 . They then package the Helm Chart and publish it as an OCI Artifact to the public.ecr.aws/aws-controllers-k8s/chart registry, using an OCI artifact tag of elasticache-v1-stable . A couple months later, the maintainer team has added a few minor, non-breaking features to their controller along with a number of bug fixes. The latest SemVer tag for the ElastiCache controller image is at v1.3.9 . The maintainer team has separately been executing long-running tests against the v1.3.2 controller image and are confident that this release is appropriate for production use. The maintainer team would update the Helm Chart in their stable Git branch to have its Deployment.spec.template.spec.containers[0].image set to public.ecr.aws/aws-controllers-k8s/controller/elasticache-v1.3.2 . They would then package this Helm Chart and push overwrite the public.ecr.aws/aws-controllers-k8s/chart:elasticache-v1-stable OCI Artifact tag to point to this newly-updated Helm Chart that refers to the v1.3.2 controller image. Maintenance Phases \u00b6 As noted above, individual ACK service controllers all use Semantic Versioning (\"X.Y.Z\") in order to signal breaking interface changes. However, each controller follows its own release cadence and each controller has a separate team of contributors that maintain the code, test the controller and determine whether the controller is stable in long-running operation. ACK service controllers having release tags within a major Semantic Version (\"X\") will be in one of four Maintenance Phases: PREVIEW GENERAL AVAILABILITY (GA) DEPRECATED NOT SUPPORTED Preview \u00b6 ACK controllers in the Preview Maintenance Phase are released for testing by users and are not recommended for production use. For Preview controllers, we ask users to submit bug reports using Github Issues and we will do our best to remediate problems in a timely manner. General Availability \u00b6 ACK controllers in the General Availability (GA) Maintenance Phase have been through long-running \"soak\" tests and are recommended for production use by the team maintaining that controller. All ACK controllers in the General Availability Maintenance Phase will have a Helm Chart with the stable artifact tag. Users who submit bug reports using Github Issues that reference a General Availability controller will have their bug reports prioritized by the contributor team maintaining that controller. Deprecated \u00b6 ACK controllers in the General Availability Maintenance Phase may move to a Deprecated Maintenance Phase after a Deprecation Warning notice has been sent out (and the controller's documentation has been updated with said deprecation notice). Controllers in Deprecated Maintenance Phase continue to receive the same level of support as controllers in the General Availability phase. Not Supported \u00b6 ACK controllers may eventually be moved into a Not Supported Maintenance Phase. A controller major version series may move from the Preview Maintenance Phase to the Not Supported Maintenance Phase at any time. This may happen if the team maintaining the controller determines it is not possible to get the controller with that major version series into a General Availability phase. A controller major version series may move from the Deprecated Maintenance Phase to the Not Supported Maintenance Phase only after a 1-year deprecation period has elapsed .","title":"Releases, Versioning and Maintenance Phases"},{"location":"releases/#releases-versioning-and-maintenance-phases","text":"Service controllers are built in separate source code repositories. Below, you will find a description of the Project Stages that a service controller repository goes through on its way to being released. A controller that has reached the RELEASED project stage will have a set of release artifacts, including binary Docker images for the controller and a Helm Chart that installs the controller into a target Kubernetes cluster. Read more below about our Releases and Versioning policy . Finally, we have a set of documented Maintenance Phases that clearly outline our support stance for service controllers that have been released.","title":"Releases, Versioning and Maintenance Phases"},{"location":"releases/#project-stages","text":"The controller's \"project stage\" describes how far along the controller is towards being released: PROPOSED -> PLANNED -> IN PROGRESS -> RELEASED","title":"Project Stages"},{"location":"releases/#proposed","text":"The PROPOSED stage indicates that there is expressed interest in supporting an AWS service in ACK. At this stage, there will be a Github Issue and/or a Github Project for tracking the creation of the ACK service controller for the service. The GitHub Issue WILL NOT be associated with a GitHub Milestone.","title":"PROPOSED"},{"location":"releases/#planned","text":"The PLANNED stage indicates that we plan to make a controller for this service available in ACK. At this stage, there WILL BE a GitHub Milestone that tracks progress towards the release of the controller.","title":"PLANNED"},{"location":"releases/#in-progress","text":"The IN PROGRESS stage indicates that the ACK service controller for the AWS service is actively being built in preparation for a release of that ACK service controller. In the IN PROGRESS stage we identify those AWS service API resources that will be supported by the controller and generate the code that manages the lifecycle of these resources. !!! note \"what do we mean by 'AWS service API resources'? An AWS service API resource is a top-level object that can be created by a particular AWS service API. For example, an SNS Topic or an S3 Bucket. Some service APIs have multiple top-level resources; SNS, for instance, has Topic, PlatformApplication and PlatformEndpoint top-level resources that may be created.","title":"IN PROGRESS"},{"location":"releases/#released","text":"The RELEASED project stage indicates that the ACK service controller source repository has had a Semantic Versioning Git tag applied and that both a Docker image and Helm Chart have been built and published to the ECR Public repositories for ACK. Once a service controller reaches the RELEASED project stage, that does not mean that there can never be any changes or additions to the Custom Resource Definitions (CRDs) or public interfaces exposed by that service controller. The RELEASED project stage is simply an indication that there is at least one SemVer-tagged binary release of the controller. Consumers should look to the SemVer release tag as an indication of whether code included in that release introduces new breaking (major version increment) or non-breaking features (minor version increment) or simply bug fixes (patch version increment). Consumers should see release notes for a release tag for a full description of changes included in that release.","title":"RELEASED"},{"location":"releases/#releases-and-versioning","text":"Important ACK does not have a single release status or version. Different components within the ACK project have different release cadences, versions and statuses. Please read the information below before installing any ACK component. Service controllers in ACK use Semantic Versioning to indicate whether changes included in a particular binary release introduce features or bug fixes and whether or not features break backwards compatibility for public APIs and interfaces. There are two release artifacts produced when an ACK service controller is released: a binary Docker image with the controller and a Helm Chart that installs the controller into a target Kubernetes cluster. Both these artifacts will have tags that correspond to the Semantic Version Git tag applied against the source code repository for the controller. Service controllers may have a Stable Helm Chart that will install a version of the service controller binary that the maintainer team is confident will hold up to production use.","title":"Releases and Versioning"},{"location":"releases/#semantic-versioning","text":"ACK is a collection of custom Kubernetes controllers, one for each supported AWS API. Each ACK controller is composed of an ACK common runtime and Go code that links the Kubernetes API and the AWS API. Much of this Go code is generated by the ack-generate tool; some of the Go code is hand-crafted. All code components in ACK use Semantic Versioning (SemVer) as a signal to consumers whether public interfaces or APIs have breaking changes. When an ACK component is released , a Git tag containing a SemVer (X.Y.Z) is created on the component's source repository. If the commits to the source repository in between the last Git tag and the commit being tagged have introduced changes that break public-facing APIs or interfaces, the SemVer will have its major version (\"X\") incremented. If the commits introduce functionality that does not break interfaces or APIs, the minor version (\"Y\") will be incremented. If the commits simply fix bugs and do not introduce any features or interface changes, the patch version (\"Z\") will be incremented. Releases of any ACK component that have a zero major release number (e.g. v0.0.2 ) may have breaking changes to the public API or interfaces exposed by that component. This is by design, and per the Semantic Versioning specification : Major version zero (0.y.z) is for initial development. Anything MAY change at any time. The public API SHOULD NOT be considered stable. For ACK components that have a binary distributable -- i.e. a Docker image -- the creation of a new SemVer Git tag on the source code repository will trigger automatic building and publishing of a Docker image with an image tag including the SemVer version. For example, if a Git tag of v1.2.6 was created on the github.com/aws-controllers-k8s/s3-controller repository, a Docker image with a tag s3-v1.2.6 would be published to the aws-controllers-k8s/controller ECR repository. Note Binaries for ACK components are published in our Amazon ECR Public registry . For ACK components that have a Helm Chart distributable -- i.e. an ACK service controller -- the creation of a new SemVer Git tag on the source code repository will trigger automatic building and publishing of a Helm Chart with an artifact tag including the SemVer version. For example, a Git tag of v1.2.6 on the github.com/aws-controllers-k8s/s3-controller repository means a Helm chart with a tag s3-v1.2.6 would be published to the aws-controllers-k8s/chart ECR repository.","title":"Semantic Versioning"},{"location":"releases/#a-word-about-dependencies","text":"Each service-specific ACK controller -- e.g. the ElastiCache ACK controller -- depends on a specific version of the ACK common runtime. This dependency is specified in the controller's go.mod file. The ACK code generator that produces Go code for service controllers depends on a specific version of the ACK common runtime. dependency between the code generator and common runtime The ACK code generator depends on the ACK common runtime in a unique way: the Go code that the ACK code generator produces adheres to a specific version of the ACK common runtime. Even though no Go code in the ACK code generator actually imports the ACK common runtime, this dependency exists because the Go code produced by the templates inside the code generator imports the ACK common runtime. In order to make this Go code dependency more strict, we have a test package inside the ACK code generator that imports the ACK common runtime. In this way, we're able to include a version-specific dependency line in the ACK code generator's go.mod file, thereby allowing Go's module infrastructure to pin the dependency between the code generator and the common runtime.","title":"A Word About Dependencies"},{"location":"releases/#stable-helm-charts","text":"Tip We recommend using Helm to install an ACK service controller. Some ACK service controllers will have Helm Charts with a $SERVICE-v$MAJOR_VERSION-stable tag, referred from here out as just a \" stable artifact tag\". There will only be one of these tags for the ACK service controller in a major version series . For example, the stable artifact tag for the ElastiCache ACK service controller's \"v1\" major version series would be elasticache-v1-stable . This stable artifact tag points to a Helm chart that has configuration values that have been tested with a specific SemVer Docker image. Typically these tests are \"soak\" tests and allow the team maintaining that ACK controller's source code to have a high degree of confidence in the controller's long-running operation. Note Please note that not all ACK service controllers will have a Helm chart with a stable artifact tag. Furthermore, there will only ever be a single stable Helm Chart tag per major version series of a controller . This stable Helm Chart tag (an OCI Artifact tag) will point to different Helm Chart packages over time. From time to time, the maintainer team for a service controller may update the configuration values and associated SemVer Docker image tag for the controller binary to point to a newer image. For example, consider the ElastiCache ACK service controller maintainer team has executed a series of long-running tests of the controller image tagged with the elasticache-v1.2.6 SemVer tag. The maintainer team is confident that the controller is stable for production use. In the stable Git branch of the ElastiCache service controller's source repository, the team would update the Helm Chart's Deployment, setting the Deployment.spec.template.spec.containers[0].image to public.ecr.aws/aws-controllers-k8s/controller/elasticache-v1.2.6 . They then package the Helm Chart and publish it as an OCI Artifact to the public.ecr.aws/aws-controllers-k8s/chart registry, using an OCI artifact tag of elasticache-v1-stable . A couple months later, the maintainer team has added a few minor, non-breaking features to their controller along with a number of bug fixes. The latest SemVer tag for the ElastiCache controller image is at v1.3.9 . The maintainer team has separately been executing long-running tests against the v1.3.2 controller image and are confident that this release is appropriate for production use. The maintainer team would update the Helm Chart in their stable Git branch to have its Deployment.spec.template.spec.containers[0].image set to public.ecr.aws/aws-controllers-k8s/controller/elasticache-v1.3.2 . They would then package this Helm Chart and push overwrite the public.ecr.aws/aws-controllers-k8s/chart:elasticache-v1-stable OCI Artifact tag to point to this newly-updated Helm Chart that refers to the v1.3.2 controller image.","title":"Stable Helm Charts"},{"location":"releases/#maintenance-phases","text":"As noted above, individual ACK service controllers all use Semantic Versioning (\"X.Y.Z\") in order to signal breaking interface changes. However, each controller follows its own release cadence and each controller has a separate team of contributors that maintain the code, test the controller and determine whether the controller is stable in long-running operation. ACK service controllers having release tags within a major Semantic Version (\"X\") will be in one of four Maintenance Phases: PREVIEW GENERAL AVAILABILITY (GA) DEPRECATED NOT SUPPORTED","title":"Maintenance Phases"},{"location":"releases/#preview","text":"ACK controllers in the Preview Maintenance Phase are released for testing by users and are not recommended for production use. For Preview controllers, we ask users to submit bug reports using Github Issues and we will do our best to remediate problems in a timely manner.","title":"Preview"},{"location":"releases/#general-availability","text":"ACK controllers in the General Availability (GA) Maintenance Phase have been through long-running \"soak\" tests and are recommended for production use by the team maintaining that controller. All ACK controllers in the General Availability Maintenance Phase will have a Helm Chart with the stable artifact tag. Users who submit bug reports using Github Issues that reference a General Availability controller will have their bug reports prioritized by the contributor team maintaining that controller.","title":"General Availability"},{"location":"releases/#deprecated","text":"ACK controllers in the General Availability Maintenance Phase may move to a Deprecated Maintenance Phase after a Deprecation Warning notice has been sent out (and the controller's documentation has been updated with said deprecation notice). Controllers in Deprecated Maintenance Phase continue to receive the same level of support as controllers in the General Availability phase.","title":"Deprecated"},{"location":"releases/#not-supported","text":"ACK controllers may eventually be moved into a Not Supported Maintenance Phase. A controller major version series may move from the Preview Maintenance Phase to the Not Supported Maintenance Phase at any time. This may happen if the team maintaining the controller determines it is not possible to get the controller with that major version series into a General Availability phase. A controller major version series may move from the Deprecated Maintenance Phase to the Not Supported Maintenance Phase only after a 1-year deprecation period has elapsed .","title":"Not Supported"},{"location":"services/","text":"Services \u00b6 The following AWS service APIs have service controllers included in ACK or have controllers in one of our several project stages . ACK controllers that have reached the RELEASED project stage will also be in one of our maintenance phases . For details, including a list of planned AWS service APIs, see the Service Controller Release Roadmap : IMPORTANT There is no single release of the ACK project. The ACK project contains a series of service controllers, one for each AWS service API. Each individual ACK service controller is released separately. Please see the release documentation for information on how we version and release ACK service controllers. AWS Service Project Stage Maintenance Phase Next Milestone Amazon ACM PROPOSED Amazon API Gateway V2 RELEASED PREVIEW https://github.com/aws-controllers-k8s/community/milestone/15 Amazon CloudFront Distribution PLANNED https://github.com/aws-controllers-k8s/community/milestone/14 Amazon DynamoDB RELEASED PREVIEW Amazon ECR RELEASED PREVIEW Amazon EFS PROPOSED Amazon EKS PLANNED https://github.com/aws-controllers-k8s/community/milestone/7 Amazon ElastiCache RELEASED PREVIEW https://github.com/aws-controllers-k8s/community/milestone/9 Amazon Elasticsearch PROPOSED Amazon EC2 VPC PROPOSED AWS IAM PROPOSED AWS Lambda IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/10 AWS Kinesis PROPOSED Amazon KMS IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/18 Amazon MQ IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/12 Amazon MSK PLANNED https://github.com/aws-controllers-k8s/community/milestone/13 Amazon RDS IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/8 Amazon Route53 PROPOSED Amazon SageMaker IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/11 Amazon SNS RELEASED PREVIEW Amazon SQS IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/6 AWS Step Functions RELEASED PREVIEW Amazon S3 RELEASED PREVIEW Don't see a service listed? If you don't see a particular AWS service listed, feel free to propose it ! Amazon ACM \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/482 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/acm/ Amazon API Gateway v2 \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/207 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/15 AWS service documentation: https://aws.amazon.com/api-gateway/ ACK service controller: https://github.com/aws-controllers-k8s/apigatewayv2-controller Amazon CloudFront Distribution \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/249 Next milestone: https://github.com/aws-controllers-k8s/community/milestone/14 Current project stage: PLANNED AWS service documentation: https://aws.amazon.com/cloudfront/ Amazon DynamoDB \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/206 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/16 AWS service documentation: https://aws.amazon.com/dynamodb/ ACK service controller: https://github.com/aws-controllers-k8s/dynamodb-controller Amazon ECR \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/208 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/16 AWS service documentation: https://aws.amazon.com/ecr/ ACK service controller: https://github.com/aws-controllers-k8s/ecr-controller Amazon EFS \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/328 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/efs/ Amazon EKS \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/16 Current project stage: PLANNED AWS service documentation: https://aws.amazon.com/eks/ Amazon ElastiCache \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/240 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/9 AWS service documentation: https://aws.amazon.com/elasticache/ ACK service controller: https://github.com/aws-controllers-k8s/elasticache-controller Amazon Elasticsearch \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/503 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/elasticsearch-service/ Amazon EC2 VPC \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/489 Current project stage: PROPOSED AWS service documentation: https://docs.aws.amazon.com/vpc/ AWS IAM \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/222 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/iam/ AWS Lambda \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/238 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/10 AWS service documentation: https://aws.amazon.com/lambda/ Amazon Kinesis \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/235 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/kinesis/ AWS KMS \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/491 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/18 AWS service documentation: https://aws.amazon.com/kms/ ACK service controller: https://github.com/aws-controllers-k8s/kms-controller Amazon MQ \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/390 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/12 AWS service documentation: https://aws.amazon.com/amazon-mq/ ACK service controller: https://github.com/aws-controllers-k8s/mq-controller Amazon MSK \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/348 Current project stage: PLANNED Next milestone: https://github.com/aws-controllers-k8s/community/milestone/13 AWS service documentation: https://aws.amazon.com/msk/ Amazon RDS \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/237 Current project stage: PLANNED Next milestone: https://github.com/aws-controllers-k8s/community/milestone/8 AWS service documentation: https://aws.amazon.com/rds/ Amazon Route53 \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/480 Current project stage: PROPOSED AWS service documentation: https://docs.aws.amazon.com/Route53/ Amazon SageMaker \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/385 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/11 AWS service documentation: https://aws.amazon.com/sagemaker/ ACK service controller: https://github.com/aws-controllers-k8s/sagemaker-controller Amazon SNS \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/202 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/17 AWS service documentation: https://aws.amazon.com/sns/ ACK service controller: https://github.com/aws-controllers-k8s/sns-controller Amazon SQS \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/205 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/6 AWS service documentation: https://aws.amazon.com/sqs/ ACK service controller: https://github.com/aws-controllers-k8s/sqs-controller AWS Step Functions \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/239 Current project stage: RELEASED Current maintenance phase: PREVIEW AWS service documentation: https://aws.amazon.com/step-functions/ ACK service controller: https://github.com/aws-controllers-k8s/sfn-controller Amazon S3 \u00b6 Proposed: https://github.com/aws-controllers-k8s/community/issues/204 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/16 AWS service documentation: https://aws.amazon.com/s3/ ACK service controller: https://github.com/aws-controllers-k8s/s3-controller","title":"Services"},{"location":"services/#services","text":"The following AWS service APIs have service controllers included in ACK or have controllers in one of our several project stages . ACK controllers that have reached the RELEASED project stage will also be in one of our maintenance phases . For details, including a list of planned AWS service APIs, see the Service Controller Release Roadmap : IMPORTANT There is no single release of the ACK project. The ACK project contains a series of service controllers, one for each AWS service API. Each individual ACK service controller is released separately. Please see the release documentation for information on how we version and release ACK service controllers. AWS Service Project Stage Maintenance Phase Next Milestone Amazon ACM PROPOSED Amazon API Gateway V2 RELEASED PREVIEW https://github.com/aws-controllers-k8s/community/milestone/15 Amazon CloudFront Distribution PLANNED https://github.com/aws-controllers-k8s/community/milestone/14 Amazon DynamoDB RELEASED PREVIEW Amazon ECR RELEASED PREVIEW Amazon EFS PROPOSED Amazon EKS PLANNED https://github.com/aws-controllers-k8s/community/milestone/7 Amazon ElastiCache RELEASED PREVIEW https://github.com/aws-controllers-k8s/community/milestone/9 Amazon Elasticsearch PROPOSED Amazon EC2 VPC PROPOSED AWS IAM PROPOSED AWS Lambda IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/10 AWS Kinesis PROPOSED Amazon KMS IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/18 Amazon MQ IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/12 Amazon MSK PLANNED https://github.com/aws-controllers-k8s/community/milestone/13 Amazon RDS IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/8 Amazon Route53 PROPOSED Amazon SageMaker IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/11 Amazon SNS RELEASED PREVIEW Amazon SQS IN PROGRESS https://github.com/aws-controllers-k8s/community/milestone/6 AWS Step Functions RELEASED PREVIEW Amazon S3 RELEASED PREVIEW Don't see a service listed? If you don't see a particular AWS service listed, feel free to propose it !","title":"Services"},{"location":"services/#amazon-acm","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/482 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/acm/","title":"Amazon ACM"},{"location":"services/#amazon-api-gateway-v2","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/207 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/15 AWS service documentation: https://aws.amazon.com/api-gateway/ ACK service controller: https://github.com/aws-controllers-k8s/apigatewayv2-controller","title":"Amazon API Gateway v2"},{"location":"services/#amazon-cloudfront-distribution","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/249 Next milestone: https://github.com/aws-controllers-k8s/community/milestone/14 Current project stage: PLANNED AWS service documentation: https://aws.amazon.com/cloudfront/","title":"Amazon CloudFront Distribution"},{"location":"services/#amazon-dynamodb","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/206 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/16 AWS service documentation: https://aws.amazon.com/dynamodb/ ACK service controller: https://github.com/aws-controllers-k8s/dynamodb-controller","title":"Amazon DynamoDB"},{"location":"services/#amazon-ecr","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/208 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/16 AWS service documentation: https://aws.amazon.com/ecr/ ACK service controller: https://github.com/aws-controllers-k8s/ecr-controller","title":"Amazon ECR"},{"location":"services/#amazon-efs","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/328 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/efs/","title":"Amazon EFS"},{"location":"services/#amazon-eks","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/16 Current project stage: PLANNED AWS service documentation: https://aws.amazon.com/eks/","title":"Amazon EKS"},{"location":"services/#amazon-elasticache","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/240 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/9 AWS service documentation: https://aws.amazon.com/elasticache/ ACK service controller: https://github.com/aws-controllers-k8s/elasticache-controller","title":"Amazon ElastiCache"},{"location":"services/#amazon-elasticsearch","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/503 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/elasticsearch-service/","title":"Amazon Elasticsearch"},{"location":"services/#amazon-ec2-vpc","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/489 Current project stage: PROPOSED AWS service documentation: https://docs.aws.amazon.com/vpc/","title":"Amazon EC2 VPC"},{"location":"services/#aws-iam","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/222 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/iam/","title":"AWS IAM"},{"location":"services/#aws-lambda","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/238 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/10 AWS service documentation: https://aws.amazon.com/lambda/","title":"AWS Lambda"},{"location":"services/#amazon-kinesis","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/235 Current project stage: PROPOSED AWS service documentation: https://aws.amazon.com/kinesis/","title":"Amazon Kinesis"},{"location":"services/#aws-kms","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/491 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/18 AWS service documentation: https://aws.amazon.com/kms/ ACK service controller: https://github.com/aws-controllers-k8s/kms-controller","title":"AWS KMS"},{"location":"services/#amazon-mq","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/390 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/12 AWS service documentation: https://aws.amazon.com/amazon-mq/ ACK service controller: https://github.com/aws-controllers-k8s/mq-controller","title":"Amazon MQ"},{"location":"services/#amazon-msk","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/348 Current project stage: PLANNED Next milestone: https://github.com/aws-controllers-k8s/community/milestone/13 AWS service documentation: https://aws.amazon.com/msk/","title":"Amazon MSK"},{"location":"services/#amazon-rds","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/237 Current project stage: PLANNED Next milestone: https://github.com/aws-controllers-k8s/community/milestone/8 AWS service documentation: https://aws.amazon.com/rds/","title":"Amazon RDS"},{"location":"services/#amazon-route53","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/480 Current project stage: PROPOSED AWS service documentation: https://docs.aws.amazon.com/Route53/","title":"Amazon Route53"},{"location":"services/#amazon-sagemaker","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/385 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/11 AWS service documentation: https://aws.amazon.com/sagemaker/ ACK service controller: https://github.com/aws-controllers-k8s/sagemaker-controller","title":"Amazon SageMaker"},{"location":"services/#amazon-sns","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/202 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/17 AWS service documentation: https://aws.amazon.com/sns/ ACK service controller: https://github.com/aws-controllers-k8s/sns-controller","title":"Amazon SNS"},{"location":"services/#amazon-sqs","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/205 Current project stage: IN PROGRESS Next milestone: https://github.com/aws-controllers-k8s/community/milestone/6 AWS service documentation: https://aws.amazon.com/sqs/ ACK service controller: https://github.com/aws-controllers-k8s/sqs-controller","title":"Amazon SQS"},{"location":"services/#aws-step-functions","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/239 Current project stage: RELEASED Current maintenance phase: PREVIEW AWS service documentation: https://aws.amazon.com/step-functions/ ACK service controller: https://github.com/aws-controllers-k8s/sfn-controller","title":"AWS Step Functions"},{"location":"services/#amazon-s3","text":"Proposed: https://github.com/aws-controllers-k8s/community/issues/204 Current project stage: RELEASED Current maintenance phase: PREVIEW Next milestone: https://github.com/aws-controllers-k8s/community/milestone/16 AWS service documentation: https://aws.amazon.com/s3/ ACK service controller: https://github.com/aws-controllers-k8s/s3-controller","title":"Amazon S3"},{"location":"community/background/","text":"Background \u00b6 In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize. Existing custom controllers \u00b6 AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book. Related projects \u00b6 Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Background"},{"location":"community/background/#background","text":"In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize.","title":"Background"},{"location":"community/background/#existing-custom-controllers","text":"AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book.","title":"Existing custom controllers"},{"location":"community/background/#related-projects","text":"Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Related projects"},{"location":"community/discussions/","text":"Discussions \u00b6 For discussions, please use the #provider-aws channel on the Kubernetes Slack community .","title":"Discussions"},{"location":"community/discussions/#discussions","text":"For discussions, please use the #provider-aws channel on the Kubernetes Slack community .","title":"Discussions"},{"location":"community/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Service Broker \u00b6 Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account. Cluster API \u00b6 Question Does the planned ACK service controller for EKS replace Kubernetes Cluster API ? Answer No, the ACK service controller for EKS does not replace Kubernetes Cluster API. Cluster API does a lot of really cool things and is designed to be a generic way to create Kubernetes clusters that run anywhere. It makes some different design decisions with that goal in mind. Some differences include: Cluster API is treated as your source of truth for all infrastructure. This means things like the cluster autoscaler need to be configured to use cluster api instead of AWS cloud provider. Generic Kubernetes clusters rely on running more services in the cluster and not services from AWS. Things like metrics and logging will likely need to run inside Kubernetes instead of using services like CloudWatch. IAM permission for Cluster-API Provider AWS (CAPA) need to be more broad than the ACK service controller for EKS because CAPA is responsible for provisioning everything needed for the cluster (VPC, gateway, etc). You don't need to run all of the ACK controllers if all you want is a way to provision an EKS cluster. You can pick and choose which ACK controllers you want to deploy. With the EKS ACK controller you will get all of the configuration flexibility of the EKS API including things like managed node groups and fargate. This is because the ACK service controller for EKS is built directly from the EKS API spec and not abstracted to be a general Kubernetes cluster. cdk8s \u00b6 Question How does ACK relate to cdk8s ? Answer cdk8s is an open-source software development framework for defining Kubernetes applications and reusable abstractions using familiar programming languages and rich object-oriented APIs. You can use cdk8s to create any resource inside a Kubernetes cluster. This includes Custom Resources (CRs). All of the ACK controllers watch for specific CRs and you can generate those resources using cdk8s or any Kubernetes tooling. The two projects complement each other. cdk8s can create the Kubernetes resources and ACK uses those resources to create the AWS infrastructure. Contributing \u00b6 Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the contributor docs .","title":"FAQ"},{"location":"community/faq/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"community/faq/#service-broker","text":"Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account.","title":"Service Broker"},{"location":"community/faq/#cluster-api","text":"Question Does the planned ACK service controller for EKS replace Kubernetes Cluster API ? Answer No, the ACK service controller for EKS does not replace Kubernetes Cluster API. Cluster API does a lot of really cool things and is designed to be a generic way to create Kubernetes clusters that run anywhere. It makes some different design decisions with that goal in mind. Some differences include: Cluster API is treated as your source of truth for all infrastructure. This means things like the cluster autoscaler need to be configured to use cluster api instead of AWS cloud provider. Generic Kubernetes clusters rely on running more services in the cluster and not services from AWS. Things like metrics and logging will likely need to run inside Kubernetes instead of using services like CloudWatch. IAM permission for Cluster-API Provider AWS (CAPA) need to be more broad than the ACK service controller for EKS because CAPA is responsible for provisioning everything needed for the cluster (VPC, gateway, etc). You don't need to run all of the ACK controllers if all you want is a way to provision an EKS cluster. You can pick and choose which ACK controllers you want to deploy. With the EKS ACK controller you will get all of the configuration flexibility of the EKS API including things like managed node groups and fargate. This is because the ACK service controller for EKS is built directly from the EKS API spec and not abstracted to be a general Kubernetes cluster.","title":"Cluster API"},{"location":"community/faq/#cdk8s","text":"Question How does ACK relate to cdk8s ? Answer cdk8s is an open-source software development framework for defining Kubernetes applications and reusable abstractions using familiar programming languages and rich object-oriented APIs. You can use cdk8s to create any resource inside a Kubernetes cluster. This includes Custom Resources (CRs). All of the ACK controllers watch for specific CRs and you can generate those resources using cdk8s or any Kubernetes tooling. The two projects complement each other. cdk8s can create the Kubernetes resources and ACK uses those resources to create the AWS infrastructure.","title":"cdk8s"},{"location":"community/faq/#contributing","text":"Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the contributor docs .","title":"Contributing"},{"location":"dev-docs/api-inference/","text":"API Inference \u00b6 This document discusses how ACK introspects an AWS API model file and determines which CustomResourceDefinition s (CRDs) to construct and what the structure of those CRDs look like. The Kubernetes Resource Model \u00b6 The Kubernetes Resource Model (KRM) is a set of standards and naming conventions that govern how an Object may be created and updated. An Object includes some metadata about the object -- a GroupVersionKind (GVK), a Name , a Namespace , and zero or more Labels and Annotations . In addition to this metadata, each Object has a Spec field which is a struct that contains the desired state of the Object . Objects are typically denoted using YAML, like so: 1 2 3 4 5 6 7 8 apiVersion : s3.services.k8s.aws/v1alpha1 kind : Bucket metadata : name : my-amazing-bucket annotations : pronounced-as : boo-kay spec : name : my-amazing-bucket Manifests The YAML files containing an object definition like above are typically called manifests . Above, the Object has a GVK of \"s3.services.k8s.aws/v1alpha1:Bucket\" with an internal-to-Kubernetes Name of \"my-amazing-bucket\" and a single Annotation key/value pair \"pronounced-as: boo-kay\". The Spec field is a structure containing desired state fields about this Bucket. You can see here that there is a Spec.Name field representing the Bucket name that will be passed to the S3 CreateBucket API as the name of the Bucket. Note that the Metadata.Name field value is the same as the Spec.Name field value here, but there's nothing mandatory about this. When a Kubernetes user creates an Object , typically by passing some YAML to the kubectl create or kubectl apply CLI command, the Kubernetes API server reads the manifest and determines whether the supplied contents are valid. In order to determine if a manifest is valid, the Kubernetes API server must look up the definition of the specified GroupVersionKind . For all of the resources that ACK is concerned about, what this means is that the Kubernetes API server will search for the CustomResourceDefinition (CRD) matching the GroupVersionKind . This CRD describes the fields that comprise Object s of that particular GroupVersionKind -- called CustomResources (CRs). In the next sections we discuss: how ACK determines what will become a CRD how ACK determines the fields that go into each CRD's Spec and Status Which things become ACK Resources? \u00b6 As mentioned in the code generation documentation , ACK reads AWS API model files when generating its API types and controller implementations. These model files are JSON files contain some important information about the structure of the AWS service API, including a set of Operation definitions (commonly called \"Actions\" in the official AWS API documentation) and a set of Shape definitions. Some AWS APIs have dozens (hundreds even!) of Operations exposed by the API. Consider EC2's API. It has over 400 separate Actions . Out of all those Operations, how are we to tell which ones refer to something that we can model as a Kubernetes CustomResource ? Well, we could look at the EC2 API's list of Operations and manually decide which ones seem \"resource-y\". Operations like \"AdvertiseByoipCidr\" and \"AcceptTransitGatewayVpcAttachment\" don't seem very \"resource-y\". Operations like \"CreateKeyPair\" and \"DeleteKeyPair\", however, do seem like they would match a resource called \"KeyPair\". And this is actually how ACK decides what is a CustomResource and what isn't. It uses a simple heuristic: look through the list of Operations in the API model file and filter out the ones that start with the string \"Create\". If what comes after the word \"Create\" describes a singular noun, then we create a CustomResource of that Kind . It really is that simple. How is an ACK Resource Defined? \u00b6 Let's take a look at the CRD for ACK's S3 Bucket (the s3.services.k8s.aws/Bucket GroupKind (GK)) (snipped slightly for brevity): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 --- apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : buckets.s3.services.k8s.aws spec : group : s3.services.k8s.aws names : kind : Bucket scope : Namespaced versions : - name : v1alpha1 schema : openAPIV3Schema : description : Bucket is the Schema for the Buckets API properties : apiVersion : type : string kind : type : string metadata : type : object spec : description : BucketSpec defines the desired state of Bucket properties : acl : type : string createBucketConfiguration : properties : locationConstraint : type : string type : object grantFullControl : type : string grantRead : type : string grantReadACP : type : string grantWrite : type : string grantWriteACP : type : string name : type : string objectLockEnabledForBucket : type : boolean required : - name type : object status : description : BucketStatus defines the observed state of Bucket properties : ackResourceMetadata : properties : arn : type : string ownerAccountID : type : string required : - ownerAccountID type : object conditions : items : properties : lastTransitionTime : format : date-time type : string message : type : string reason : type : string status : type : string type : type : string required : - status - type type : object type : array location : type : string required : - ackResourceMetadata - conditions type : object type : object The above YAML representation of a CustomResourceDefinition (CRD) is actually generated from a set of Go type definitions. These Go type definitions live in each ACK service's services/$SERVICE/apis/$VERSION directory. This section of our documentation discusses how we create those Go type definitions. controller-gen crd The OpenAPIv3 Validating Schema shown above is created by the controller-gen crd CLI command and is a convenient human-readable representation of the CustomResourceDefinition . The Bucket CR's Spec field is defined above as containing a set of fields -- \"acl\", \"createBucketConfiguration\", \"name\", etc. Each field has a JSONSchema type that corresponds with the Go type from the associated field member. You will also notice that in addition to the definition of a Spec field, there is also the definition of a Status field for the Bucket CRs. Above, this Status contains fields that represent the \"observed\" state of the Bucket CRs. The above shows three fields in the Bucket's Status : ackResourceMetadata , conditions and location . You might be wondering how the ACK code generator determined which fields go into the Bucket's Spec and which fields go into the Bucket's Status ? Well, it's definitely not a manual process. Everything in ACK is code-generated and discovered by inspecting the AWS API model files. what are AWS API model files? The AWS API model files are JSON files that contain information about a particular AWS service API's Actions and Shapes. We consume the model files distributed in the aws-sdk-go project. (Look for the api-2.json files in the linked service-specific directories...) Let's take a look at a tiny bit of the AWS S3 API model file and you can start to see how we identify the things that go into the Spec and Status fields. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 { \"metadata\" :{ \"serviceId\" : \"S3\" , }, \"operations\" :{ \"CreateBucket\" :{ \"name\" : \"CreateBucket\" , \"http\" :{ \"method\" : \"PUT\" , \"requestUri\" : \"/{Bucket}\" }, \"input\" :{ \"shape\" : \"CreateBucketRequest\" }, \"output\" :{ \"shape\" : \"CreateBucketOutput\" }, }, }, \"shapes\" :{ \"BucketCannedACL\" :{ \"type\" : \"string\" , \"enum\" :[ \"private\" , \"public-read\" , \"public-read-write\" , \"authenticated-read\" ] }, \"BucketName\" :{ \"type\" : \"string\" }, \"CreateBucketConfiguration\" :{ \"type\" : \"structure\" , \"members\" :{ \"LocationConstraint\" :{ \"shape\" : \"BucketLocationConstraint\" } } }, \"CreateBucketOutput\" :{ \"type\" : \"structure\" , \"members\" :{ \"Location\" :{ \"shape\" : \"Location\" , } } }, \"CreateBucketRequest\" :{ \"type\" : \"structure\" , \"required\" :[ \"Bucket\" ], \"members\" :{ \"ACL\" :{ \"shape\" : \"BucketCannedACL\" , }, \"Bucket\" :{ \"shape\" : \"BucketName\" , }, \"CreateBucketConfiguration\" :{ \"shape\" : \"CreateBucketConfiguration\" , }, \"GrantFullControl\" :{ \"shape\" : \"GrantFullControl\" , }, \"GrantRead\" :{ \"shape\" : \"GrantRead\" , }, \"GrantReadACP\" :{ \"shape\" : \"GrantReadACP\" , }, \"GrantWrite\" :{ \"shape\" : \"GrantWrite\" , }, \"GrantWriteACP\" :{ \"shape\" : \"GrantWriteACP\" , }, \"ObjectLockEnabledForBucket\" :{ \"shape\" : \"ObjectLockEnabledForBucket\" , } }, }, } } As mentioned above, we determine what things in an API are CustomResourceDefinition s by looking for Operation s that begin with the string \"Create\" and where the remainder of the Operation name refers to a singular noun. For the S3 API, there happens to be only a single Operation that begins with the string \"Create\", and it happens to be \" CreateBucket \". And since \"Bucket\" refers to a singular noun, that is the CustomResourceDefinition that is identified by the ACK code generator. The ACK code generator writes a file apis/v1alpha1/bucket.go that contains a BucketSpec struct definition, a BucketStatus struct definition and a Bucket struct definition that ties the Spec and Status together into our CRD. In determining the structure of the s3.services.k8s.aws/Bucket CRD, the ACK code generator inspects the Shape s referred to by the \"input\" and \"output\" members of the \"CreateBucket\" Operation : \"CreateBucketRequest\" and \"CreateBucketOutput\" respectively. Determining the Spec fields \u00b6 For the BucketSpec fields, we grab members of the Input shape. The generated Go type definition for the BucketSpec ends up looking like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 // BucketSpec defines the desired state of Bucket type BucketSpec struct { ACL * string `json:\"acl,omitempty\"` CreateBucketConfiguration * CreateBucketConfiguration `json:\"createBucketConfiguration,omitempty\"` GrantFullControl * string `json:\"grantFullControl,omitempty\"` GrantRead * string `json:\"grantRead,omitempty\"` GrantReadACP * string `json:\"grantReadACP,omitempty\"` GrantWrite * string `json:\"grantWrite,omitempty\"` GrantWriteACP * string `json:\"grantWriteACP,omitempty\"` // +kubebuilder:validation:Required Name * string `json:\"name\"` ObjectLockEnabledForBucket * bool `json:\"objectLockEnabledForBucket,omitempty\"` } Let's take a closer look at the BucketSpec fields. The ACL , GrantFullControl , GrantRead , GrantReadACP , GrantWrite and GrantWriteACP fields are simple *string types. However, if we look at the CreateBucketRequest Shape definition in the API model file, we see that these fields actually are differently-named Shapes, not *string . Why is this? Well, the ACK code generator \"flattens\" some Shapes when it notices that a named Shape is just an alias for a simple scalar type (like *string ). !!! \"why *string ?\" The astute reader may be wondering why the Go type for string fields is *string and not string . The reason for this lies in aws-sdk-go . All types for all Shape members are pointer types, even when the underlying data type is a simple scalar type like bool or int . Yes, even when the field is required... Note that even though the ACL field has a Shape of BucketCannedACL , that Shape is actually just a string with a set of enumerated values. Enumerated values are collected and written out by the ACK code generator into an apis/v1alpha1/enums.go file, with content like this: 1 2 3 4 5 6 7 8 type BucketCannedACL string const ( BucketCannedACL_private BucketCannedACL = \"private\" BucketCannedACL_public_read BucketCannedACL = \"public-read\" BucketCannedACL_public_read_write BucketCannedACL = \"public-read-write\" BucketCannedACL_authenticated_read BucketCannedACL = \"authenticated-read\" ) The CreateBucketConfiguration field is of type *CreateBucketConfiguration . All this means is that the field refers to a nested struct. All struct type definitions for CRD Spec or Status field members are placed by the ACK code generator into a apis/v1alpha1/types.go file. Here is a snippet of that file that contains the type definition for the CreateBucketConfiguration struct: 1 2 3 type CreateBucketConfiguration struct { LocationConstraint * string `json:\"locationConstraint,omitempty\"` } Now, the Name field in the BucketSpec struct seems out of place, no? There is no \"Name\" member of the CreateBucketRequest Shape, so why is there a Name field in BucketSpec ? Well, this is an example of ACK's code generator using some special instructions contained in something called the generator.yaml (or \"generator config\") for the S3 service controller. Each service in the services/ directory can have a generator.yaml file that contains overrides and special instructions for how to interpret and transform parts of the service's API. Here is part of the S3 service's generator.yaml file: 1 2 3 4 5 6 7 resources : Bucket : renames : operations : CreateBucket : input_fields : Bucket : Name As you can see, the generator config for the ACK S3 service controller is renaming the CreateBucket Operation's Input Shape Bucket field to Name . We do this for some APIs to add a little consistency and a more Kubernetes-native experience for the CRDs. In Kubernetes, there is a Metadata.Name (internal Kubernetes name) and there is typically a Spec.Name field which refers to the external Name of the resource. So, in order to align the s3.services.k8s.aws/Bucket 's definition to be more Kubernetes-like, we rename the Bucket field to Name . We do this renaming for other things that produce a bit of a \" stutter \", as well as where the name of a field does not conform to Go exported name constraints or naming best practices . Determining the Status fields \u00b6 Remember that fields in a CR's Status struct are not mutable by normal Kubernetes users. Instead, these fields represent the latest observed state of a resource (instead of the desired state of that resource which is represented by fields in the CR's Spec struct). The ACK code generator takes the members of the Create Operation 's Output shape and puts those fields into the CR's Status struct. We assume that fields in the Output that have the same name as fields in the Input shape for the Create Operation refer to the resource field that was set in the Spec field and therefore are only interested in fields in the Output that are not in the Input . Looking at the BucketSpec struct definition that was generated after processing the S3 API model file, we find this : 1 2 3 4 5 6 7 8 9 10 11 12 13 // BucketStatus defines the observed state of Bucket type BucketStatus struct { // All CRs managed by ACK have a common `Status.ACKResourceMetadata` member // that is used to contain resource sync state, account ownership, // constructed ARN for the resource ACKResourceMetadata * ackv1alpha1 . ResourceMetadata `json:\"ackResourceMetadata\"` // All CRS managed by ACK have a common `Status.Conditions` member that // contains a collection of `ackv1alpha1.Condition` objects that describe // the various terminal states of the CR and its backend AWS service API // resource Conditions [] * ackv1alpha1 . Condition `json:\"conditions\"` Location * string `json:\"location,omitempty\"` } Let's discuss each of the fields shown above. First, the ACKResourceMetadata field is included in every ACK CRD's Status field . It is a pointer to a ackv1alpha1.ResourceMetadata struct. This struct contains some standard and important pieces of information about the resource, including the AWS Resource Name (ARN) and the Owner AWS Account ID. The ARN is a globally-unique identifier for the resource in AWS. The Owner AWS Account ID is the 12-digit AWS account ID that is billed for the resource. cross-account resource management The Owner AWS Account ID for a resource may be different from the AWS Account ID of the IAM Role that the ACK service controller is executing under. The Conditions field is also included in every ACK CRD's Status field. It is a slice of pointers to ackv1alpha1.Condition structs. The Condition struct is responsible for conveying information about the latest observed sync state of a resource, including any terminal condition states that cause the resource to be \"unsyncable\". Next is the Location field. This field gets its definition from the S3 CreateBucketOutput.Location field: 1 2 3 4 5 6 7 8 \"CreateBucketOutput\" :{ \"type\" : \"structure\" , \"members\" :{ \"Location\" :{ \"shape\" : \"Location\" , } } },","title":"API Inference"},{"location":"dev-docs/api-inference/#api-inference","text":"This document discusses how ACK introspects an AWS API model file and determines which CustomResourceDefinition s (CRDs) to construct and what the structure of those CRDs look like.","title":"API Inference"},{"location":"dev-docs/api-inference/#the-kubernetes-resource-model","text":"The Kubernetes Resource Model (KRM) is a set of standards and naming conventions that govern how an Object may be created and updated. An Object includes some metadata about the object -- a GroupVersionKind (GVK), a Name , a Namespace , and zero or more Labels and Annotations . In addition to this metadata, each Object has a Spec field which is a struct that contains the desired state of the Object . Objects are typically denoted using YAML, like so: 1 2 3 4 5 6 7 8 apiVersion : s3.services.k8s.aws/v1alpha1 kind : Bucket metadata : name : my-amazing-bucket annotations : pronounced-as : boo-kay spec : name : my-amazing-bucket Manifests The YAML files containing an object definition like above are typically called manifests . Above, the Object has a GVK of \"s3.services.k8s.aws/v1alpha1:Bucket\" with an internal-to-Kubernetes Name of \"my-amazing-bucket\" and a single Annotation key/value pair \"pronounced-as: boo-kay\". The Spec field is a structure containing desired state fields about this Bucket. You can see here that there is a Spec.Name field representing the Bucket name that will be passed to the S3 CreateBucket API as the name of the Bucket. Note that the Metadata.Name field value is the same as the Spec.Name field value here, but there's nothing mandatory about this. When a Kubernetes user creates an Object , typically by passing some YAML to the kubectl create or kubectl apply CLI command, the Kubernetes API server reads the manifest and determines whether the supplied contents are valid. In order to determine if a manifest is valid, the Kubernetes API server must look up the definition of the specified GroupVersionKind . For all of the resources that ACK is concerned about, what this means is that the Kubernetes API server will search for the CustomResourceDefinition (CRD) matching the GroupVersionKind . This CRD describes the fields that comprise Object s of that particular GroupVersionKind -- called CustomResources (CRs). In the next sections we discuss: how ACK determines what will become a CRD how ACK determines the fields that go into each CRD's Spec and Status","title":"The Kubernetes Resource Model"},{"location":"dev-docs/api-inference/#which-things-become-ack-resources","text":"As mentioned in the code generation documentation , ACK reads AWS API model files when generating its API types and controller implementations. These model files are JSON files contain some important information about the structure of the AWS service API, including a set of Operation definitions (commonly called \"Actions\" in the official AWS API documentation) and a set of Shape definitions. Some AWS APIs have dozens (hundreds even!) of Operations exposed by the API. Consider EC2's API. It has over 400 separate Actions . Out of all those Operations, how are we to tell which ones refer to something that we can model as a Kubernetes CustomResource ? Well, we could look at the EC2 API's list of Operations and manually decide which ones seem \"resource-y\". Operations like \"AdvertiseByoipCidr\" and \"AcceptTransitGatewayVpcAttachment\" don't seem very \"resource-y\". Operations like \"CreateKeyPair\" and \"DeleteKeyPair\", however, do seem like they would match a resource called \"KeyPair\". And this is actually how ACK decides what is a CustomResource and what isn't. It uses a simple heuristic: look through the list of Operations in the API model file and filter out the ones that start with the string \"Create\". If what comes after the word \"Create\" describes a singular noun, then we create a CustomResource of that Kind . It really is that simple.","title":"Which things become ACK Resources?"},{"location":"dev-docs/api-inference/#how-is-an-ack-resource-defined","text":"Let's take a look at the CRD for ACK's S3 Bucket (the s3.services.k8s.aws/Bucket GroupKind (GK)) (snipped slightly for brevity): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 --- apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : buckets.s3.services.k8s.aws spec : group : s3.services.k8s.aws names : kind : Bucket scope : Namespaced versions : - name : v1alpha1 schema : openAPIV3Schema : description : Bucket is the Schema for the Buckets API properties : apiVersion : type : string kind : type : string metadata : type : object spec : description : BucketSpec defines the desired state of Bucket properties : acl : type : string createBucketConfiguration : properties : locationConstraint : type : string type : object grantFullControl : type : string grantRead : type : string grantReadACP : type : string grantWrite : type : string grantWriteACP : type : string name : type : string objectLockEnabledForBucket : type : boolean required : - name type : object status : description : BucketStatus defines the observed state of Bucket properties : ackResourceMetadata : properties : arn : type : string ownerAccountID : type : string required : - ownerAccountID type : object conditions : items : properties : lastTransitionTime : format : date-time type : string message : type : string reason : type : string status : type : string type : type : string required : - status - type type : object type : array location : type : string required : - ackResourceMetadata - conditions type : object type : object The above YAML representation of a CustomResourceDefinition (CRD) is actually generated from a set of Go type definitions. These Go type definitions live in each ACK service's services/$SERVICE/apis/$VERSION directory. This section of our documentation discusses how we create those Go type definitions. controller-gen crd The OpenAPIv3 Validating Schema shown above is created by the controller-gen crd CLI command and is a convenient human-readable representation of the CustomResourceDefinition . The Bucket CR's Spec field is defined above as containing a set of fields -- \"acl\", \"createBucketConfiguration\", \"name\", etc. Each field has a JSONSchema type that corresponds with the Go type from the associated field member. You will also notice that in addition to the definition of a Spec field, there is also the definition of a Status field for the Bucket CRs. Above, this Status contains fields that represent the \"observed\" state of the Bucket CRs. The above shows three fields in the Bucket's Status : ackResourceMetadata , conditions and location . You might be wondering how the ACK code generator determined which fields go into the Bucket's Spec and which fields go into the Bucket's Status ? Well, it's definitely not a manual process. Everything in ACK is code-generated and discovered by inspecting the AWS API model files. what are AWS API model files? The AWS API model files are JSON files that contain information about a particular AWS service API's Actions and Shapes. We consume the model files distributed in the aws-sdk-go project. (Look for the api-2.json files in the linked service-specific directories...) Let's take a look at a tiny bit of the AWS S3 API model file and you can start to see how we identify the things that go into the Spec and Status fields. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 { \"metadata\" :{ \"serviceId\" : \"S3\" , }, \"operations\" :{ \"CreateBucket\" :{ \"name\" : \"CreateBucket\" , \"http\" :{ \"method\" : \"PUT\" , \"requestUri\" : \"/{Bucket}\" }, \"input\" :{ \"shape\" : \"CreateBucketRequest\" }, \"output\" :{ \"shape\" : \"CreateBucketOutput\" }, }, }, \"shapes\" :{ \"BucketCannedACL\" :{ \"type\" : \"string\" , \"enum\" :[ \"private\" , \"public-read\" , \"public-read-write\" , \"authenticated-read\" ] }, \"BucketName\" :{ \"type\" : \"string\" }, \"CreateBucketConfiguration\" :{ \"type\" : \"structure\" , \"members\" :{ \"LocationConstraint\" :{ \"shape\" : \"BucketLocationConstraint\" } } }, \"CreateBucketOutput\" :{ \"type\" : \"structure\" , \"members\" :{ \"Location\" :{ \"shape\" : \"Location\" , } } }, \"CreateBucketRequest\" :{ \"type\" : \"structure\" , \"required\" :[ \"Bucket\" ], \"members\" :{ \"ACL\" :{ \"shape\" : \"BucketCannedACL\" , }, \"Bucket\" :{ \"shape\" : \"BucketName\" , }, \"CreateBucketConfiguration\" :{ \"shape\" : \"CreateBucketConfiguration\" , }, \"GrantFullControl\" :{ \"shape\" : \"GrantFullControl\" , }, \"GrantRead\" :{ \"shape\" : \"GrantRead\" , }, \"GrantReadACP\" :{ \"shape\" : \"GrantReadACP\" , }, \"GrantWrite\" :{ \"shape\" : \"GrantWrite\" , }, \"GrantWriteACP\" :{ \"shape\" : \"GrantWriteACP\" , }, \"ObjectLockEnabledForBucket\" :{ \"shape\" : \"ObjectLockEnabledForBucket\" , } }, }, } } As mentioned above, we determine what things in an API are CustomResourceDefinition s by looking for Operation s that begin with the string \"Create\" and where the remainder of the Operation name refers to a singular noun. For the S3 API, there happens to be only a single Operation that begins with the string \"Create\", and it happens to be \" CreateBucket \". And since \"Bucket\" refers to a singular noun, that is the CustomResourceDefinition that is identified by the ACK code generator. The ACK code generator writes a file apis/v1alpha1/bucket.go that contains a BucketSpec struct definition, a BucketStatus struct definition and a Bucket struct definition that ties the Spec and Status together into our CRD. In determining the structure of the s3.services.k8s.aws/Bucket CRD, the ACK code generator inspects the Shape s referred to by the \"input\" and \"output\" members of the \"CreateBucket\" Operation : \"CreateBucketRequest\" and \"CreateBucketOutput\" respectively.","title":"How is an ACK Resource Defined?"},{"location":"dev-docs/api-inference/#determining-the-spec-fields","text":"For the BucketSpec fields, we grab members of the Input shape. The generated Go type definition for the BucketSpec ends up looking like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 // BucketSpec defines the desired state of Bucket type BucketSpec struct { ACL * string `json:\"acl,omitempty\"` CreateBucketConfiguration * CreateBucketConfiguration `json:\"createBucketConfiguration,omitempty\"` GrantFullControl * string `json:\"grantFullControl,omitempty\"` GrantRead * string `json:\"grantRead,omitempty\"` GrantReadACP * string `json:\"grantReadACP,omitempty\"` GrantWrite * string `json:\"grantWrite,omitempty\"` GrantWriteACP * string `json:\"grantWriteACP,omitempty\"` // +kubebuilder:validation:Required Name * string `json:\"name\"` ObjectLockEnabledForBucket * bool `json:\"objectLockEnabledForBucket,omitempty\"` } Let's take a closer look at the BucketSpec fields. The ACL , GrantFullControl , GrantRead , GrantReadACP , GrantWrite and GrantWriteACP fields are simple *string types. However, if we look at the CreateBucketRequest Shape definition in the API model file, we see that these fields actually are differently-named Shapes, not *string . Why is this? Well, the ACK code generator \"flattens\" some Shapes when it notices that a named Shape is just an alias for a simple scalar type (like *string ). !!! \"why *string ?\" The astute reader may be wondering why the Go type for string fields is *string and not string . The reason for this lies in aws-sdk-go . All types for all Shape members are pointer types, even when the underlying data type is a simple scalar type like bool or int . Yes, even when the field is required... Note that even though the ACL field has a Shape of BucketCannedACL , that Shape is actually just a string with a set of enumerated values. Enumerated values are collected and written out by the ACK code generator into an apis/v1alpha1/enums.go file, with content like this: 1 2 3 4 5 6 7 8 type BucketCannedACL string const ( BucketCannedACL_private BucketCannedACL = \"private\" BucketCannedACL_public_read BucketCannedACL = \"public-read\" BucketCannedACL_public_read_write BucketCannedACL = \"public-read-write\" BucketCannedACL_authenticated_read BucketCannedACL = \"authenticated-read\" ) The CreateBucketConfiguration field is of type *CreateBucketConfiguration . All this means is that the field refers to a nested struct. All struct type definitions for CRD Spec or Status field members are placed by the ACK code generator into a apis/v1alpha1/types.go file. Here is a snippet of that file that contains the type definition for the CreateBucketConfiguration struct: 1 2 3 type CreateBucketConfiguration struct { LocationConstraint * string `json:\"locationConstraint,omitempty\"` } Now, the Name field in the BucketSpec struct seems out of place, no? There is no \"Name\" member of the CreateBucketRequest Shape, so why is there a Name field in BucketSpec ? Well, this is an example of ACK's code generator using some special instructions contained in something called the generator.yaml (or \"generator config\") for the S3 service controller. Each service in the services/ directory can have a generator.yaml file that contains overrides and special instructions for how to interpret and transform parts of the service's API. Here is part of the S3 service's generator.yaml file: 1 2 3 4 5 6 7 resources : Bucket : renames : operations : CreateBucket : input_fields : Bucket : Name As you can see, the generator config for the ACK S3 service controller is renaming the CreateBucket Operation's Input Shape Bucket field to Name . We do this for some APIs to add a little consistency and a more Kubernetes-native experience for the CRDs. In Kubernetes, there is a Metadata.Name (internal Kubernetes name) and there is typically a Spec.Name field which refers to the external Name of the resource. So, in order to align the s3.services.k8s.aws/Bucket 's definition to be more Kubernetes-like, we rename the Bucket field to Name . We do this renaming for other things that produce a bit of a \" stutter \", as well as where the name of a field does not conform to Go exported name constraints or naming best practices .","title":"Determining the Spec fields"},{"location":"dev-docs/api-inference/#determining-the-status-fields","text":"Remember that fields in a CR's Status struct are not mutable by normal Kubernetes users. Instead, these fields represent the latest observed state of a resource (instead of the desired state of that resource which is represented by fields in the CR's Spec struct). The ACK code generator takes the members of the Create Operation 's Output shape and puts those fields into the CR's Status struct. We assume that fields in the Output that have the same name as fields in the Input shape for the Create Operation refer to the resource field that was set in the Spec field and therefore are only interested in fields in the Output that are not in the Input . Looking at the BucketSpec struct definition that was generated after processing the S3 API model file, we find this : 1 2 3 4 5 6 7 8 9 10 11 12 13 // BucketStatus defines the observed state of Bucket type BucketStatus struct { // All CRs managed by ACK have a common `Status.ACKResourceMetadata` member // that is used to contain resource sync state, account ownership, // constructed ARN for the resource ACKResourceMetadata * ackv1alpha1 . ResourceMetadata `json:\"ackResourceMetadata\"` // All CRS managed by ACK have a common `Status.Conditions` member that // contains a collection of `ackv1alpha1.Condition` objects that describe // the various terminal states of the CR and its backend AWS service API // resource Conditions [] * ackv1alpha1 . Condition `json:\"conditions\"` Location * string `json:\"location,omitempty\"` } Let's discuss each of the fields shown above. First, the ACKResourceMetadata field is included in every ACK CRD's Status field . It is a pointer to a ackv1alpha1.ResourceMetadata struct. This struct contains some standard and important pieces of information about the resource, including the AWS Resource Name (ARN) and the Owner AWS Account ID. The ARN is a globally-unique identifier for the resource in AWS. The Owner AWS Account ID is the 12-digit AWS account ID that is billed for the resource. cross-account resource management The Owner AWS Account ID for a resource may be different from the AWS Account ID of the IAM Role that the ACK service controller is executing under. The Conditions field is also included in every ACK CRD's Status field. It is a slice of pointers to ackv1alpha1.Condition structs. The Condition struct is responsible for conveying information about the latest observed sync state of a resource, including any terminal condition states that cause the resource to be \"unsyncable\". Next is the Location field. This field gets its definition from the S3 CreateBucketOutput.Location field: 1 2 3 4 5 6 7 8 \"CreateBucketOutput\" :{ \"type\" : \"structure\" , \"members\" :{ \"Location\" :{ \"shape\" : \"Location\" , } } },","title":"Determining the Status fields"},{"location":"dev-docs/building-controller/","text":"Building an ACK service controller \u00b6 This document continues our contributor-focused discussion by explaining how to build or regenerate an ACK service controller. Prerequisites \u00b6 You should have forked the github.com/aws-controllers-k8s/code-generator repository and git clone 'd it locally when setting up your development environment, With the prerequisites out of the way, let's move on to the first step: building the code generator. Build code generator \u00b6 Building an ACK service controller (or regenerating an existing one from a newer API model file) requires the ack-generate binary, which is the main code generator CLI tool. To build the latest ack-generate binary, execute the following command from the root directory of the github.com/aws-controllers-k8s/code-generator source repository: 1 make build-ack-generate One-off build You only have to do this once, overall. In other words: unless we change something upstream in terms of the code generation process, this is a one-off operation. Internally, the Makefile executes an go build here. Don't worry if you forget this step, the script in the next step will complain with a message along the line of ERROR: Unable to find an ack-generate binary and will give you another opportunity to rectify the situation. Build an ACK service controller \u00b6 Now that we have the basic code generation step done we will create the respective ACK service controller and its supporting artifacts. So first you have to select a service that you want to build and test. You do that by setting the SERVICE environment variable. Let's say we want to test the S3 service (creating an S3 bucket), so we would execute the following: 1 export SERVICE = s3 Now we are in a position to generate the ACK service controller for the S3 API. 1 make build-controller SERVICE=$SERVICE By default, running make build-controller will output the generated code to ACK service controller for S3's source code repository (the $GOPATH/src/github.com/aws-controllers-k8s/s3-controller directory). You can override this behaviour with the SERVICE_CONTROLLER_SOURCE_PATH environment variable. Handle controller-gen: command not found If you run into the controller-gen: command not found message when executing make build-controller then you want to check if the controller-gen binary is available in $GOPATH/bin , also ensure that $GOPATH/bin is part of your $PATH , see also #234 . You can also install the required version of controller-gen using the scripts/install-controller-gen.sh helper script. In addition to the ACK service controller code, above generates the custom resource definition (CRD) manifests as well as the necessary RBAC settings using the /scripts/build-controller.sh . Next Steps \u00b6 Now that we have the generation part completed, we want to see if the generated artifacts indeed are able to create an S3 bucket for us. Learn about how to run e2e tests for an ACK controller .","title":"Building controller"},{"location":"dev-docs/building-controller/#building-an-ack-service-controller","text":"This document continues our contributor-focused discussion by explaining how to build or regenerate an ACK service controller.","title":"Building an ACK service controller"},{"location":"dev-docs/building-controller/#prerequisites","text":"You should have forked the github.com/aws-controllers-k8s/code-generator repository and git clone 'd it locally when setting up your development environment, With the prerequisites out of the way, let's move on to the first step: building the code generator.","title":"Prerequisites"},{"location":"dev-docs/building-controller/#build-code-generator","text":"Building an ACK service controller (or regenerating an existing one from a newer API model file) requires the ack-generate binary, which is the main code generator CLI tool. To build the latest ack-generate binary, execute the following command from the root directory of the github.com/aws-controllers-k8s/code-generator source repository: 1 make build-ack-generate One-off build You only have to do this once, overall. In other words: unless we change something upstream in terms of the code generation process, this is a one-off operation. Internally, the Makefile executes an go build here. Don't worry if you forget this step, the script in the next step will complain with a message along the line of ERROR: Unable to find an ack-generate binary and will give you another opportunity to rectify the situation.","title":"Build code generator"},{"location":"dev-docs/building-controller/#build-an-ack-service-controller","text":"Now that we have the basic code generation step done we will create the respective ACK service controller and its supporting artifacts. So first you have to select a service that you want to build and test. You do that by setting the SERVICE environment variable. Let's say we want to test the S3 service (creating an S3 bucket), so we would execute the following: 1 export SERVICE = s3 Now we are in a position to generate the ACK service controller for the S3 API. 1 make build-controller SERVICE=$SERVICE By default, running make build-controller will output the generated code to ACK service controller for S3's source code repository (the $GOPATH/src/github.com/aws-controllers-k8s/s3-controller directory). You can override this behaviour with the SERVICE_CONTROLLER_SOURCE_PATH environment variable. Handle controller-gen: command not found If you run into the controller-gen: command not found message when executing make build-controller then you want to check if the controller-gen binary is available in $GOPATH/bin , also ensure that $GOPATH/bin is part of your $PATH , see also #234 . You can also install the required version of controller-gen using the scripts/install-controller-gen.sh helper script. In addition to the ACK service controller code, above generates the custom resource definition (CRD) manifests as well as the necessary RBAC settings using the /scripts/build-controller.sh .","title":"Build an ACK service controller"},{"location":"dev-docs/building-controller/#next-steps","text":"Now that we have the generation part completed, we want to see if the generated artifacts indeed are able to create an S3 bucket for us. Learn about how to run e2e tests for an ACK controller .","title":"Next Steps"},{"location":"dev-docs/code-generation/","text":"Code generation \u00b6 In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services. Options considered \u00b6 To generate custom resource (definitions) and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ACK controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ACK is a collection of controllers that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community. Our approach \u00b6 We ended up with a hybrid custom+controller-runtime, using multiple phases of code generation: The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. We will use the model files from the aws-sdk-go source repository as our source of truth and use the aws-sdk-go/private/model/api Go package to navigate that model. Note This step is the ack-generate apis command. After generating Kubernetes API type definitions for the top-level resources exposed by the AWS API, we then need to generate the \"DeepCopy\" interface implementations that enable those top-level resources and type definitions to be used by the Kubernetes runtime package (it defines an interface called runtime.Object that requires certain methods that copy the object and its component parts). Note This step runs the controller-gen object command Next, we generate the custom resource definition (CRD) configuration files, one for each top-level resource identified in earlier steps. Note This step runs the controller-gen crd command Next, we generate the actual implementation of the ACK controller for the target service. This step uses a set of templates and code in the pkg/model Go package to construct the service-specific resource management and linkage with the aws-sdk-go client for the service. Along with these controller implementation Go files, this step also outputs a set of Kubernetes configuration files for the Deployment and the ClusterRoleBinding of the Role created in the next step. Note This step runs the ack-generate controller command Finally, we generate the configuration file for a Kubernetes Role that the Kubernetes Pod (running in a Kubernetes Deployment ) running the ACK service controller. This Role needs to have permissions to read and write CRs of the Kind that the service controller manages. Note This step runs the controller-gen rbac command Crossplane Provider Generation \u00b6 We have experimental support for generating API types and controller code for AWS services to be used in Crossplane AWS Provider. To try it out, you can run the following command: 1 2 3 go run -tags codegen cmd/ack-generate/main.go crossplane apis ecr --provider-dir <directory for provider> cd <directory for provider> go generate ./...","title":"Code generation"},{"location":"dev-docs/code-generation/#code-generation","text":"In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services.","title":"Code generation"},{"location":"dev-docs/code-generation/#options-considered","text":"To generate custom resource (definitions) and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ACK controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ACK is a collection of controllers that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community.","title":"Options considered"},{"location":"dev-docs/code-generation/#our-approach","text":"We ended up with a hybrid custom+controller-runtime, using multiple phases of code generation: The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. We will use the model files from the aws-sdk-go source repository as our source of truth and use the aws-sdk-go/private/model/api Go package to navigate that model. Note This step is the ack-generate apis command. After generating Kubernetes API type definitions for the top-level resources exposed by the AWS API, we then need to generate the \"DeepCopy\" interface implementations that enable those top-level resources and type definitions to be used by the Kubernetes runtime package (it defines an interface called runtime.Object that requires certain methods that copy the object and its component parts). Note This step runs the controller-gen object command Next, we generate the custom resource definition (CRD) configuration files, one for each top-level resource identified in earlier steps. Note This step runs the controller-gen crd command Next, we generate the actual implementation of the ACK controller for the target service. This step uses a set of templates and code in the pkg/model Go package to construct the service-specific resource management and linkage with the aws-sdk-go client for the service. Along with these controller implementation Go files, this step also outputs a set of Kubernetes configuration files for the Deployment and the ClusterRoleBinding of the Role created in the next step. Note This step runs the ack-generate controller command Finally, we generate the configuration file for a Kubernetes Role that the Kubernetes Pod (running in a Kubernetes Deployment ) running the ACK service controller. This Role needs to have permissions to read and write CRs of the Kind that the service controller manages. Note This step runs the controller-gen rbac command","title":"Our approach"},{"location":"dev-docs/code-generation/#crossplane-provider-generation","text":"We have experimental support for generating API types and controller code for AWS services to be used in Crossplane AWS Provider. To try it out, you can run the following command: 1 2 3 go run -tags codegen cmd/ack-generate/main.go crossplane apis ecr --provider-dir <directory for provider> cd <directory for provider> go generate ./...","title":"Crossplane Provider Generation"},{"location":"dev-docs/overview/","text":"Overview \u00b6 This section of the docs is for ACK contributors. Code Organization \u00b6 ACK is a collection of source repositories containing a common runtime and type system, a code generator and individual service controllers that manage resources in a specific AWS API. github.com/aws-controllers-k8s/community docs and common tests (this repo) github.com/aws-controllers-k8s/runtime : common ACK runtime and types github.com/aws-controllers-k8s/code-generator : the code generator and templates github.com/aws-controllers-k8s/$SERVICE-controller : individual ACK controllers for AWS services. github.com/aws-controllers-k8s/community (this repo) \u00b6 The github.com/aws-controllers-k8s/community source code repository (this repo) contains the common test scripts and documentation that gets published to https://aws-controllers-k8s.github.io/community/. github.com/aws-controllers-k8s/runtime \u00b6 The github.com/aws-controllers-k8s/runtime source code repository contains the common ACK controller runtime ( /pkg/runtime , /pkg/types ) and core public Kubernetes API types ( /apis/core ). github.com/aws-controllers-k8s/code-generator \u00b6 The github.com/aws-controllers-k8s/code-generator source code repository contains the ack-generate CLI tool ( /cmd/ack-generate ), the Go packages that are used in API inference and code generation ( /pkg/generate , /pkg/model ) and Bash scripts to build an ACK service controller ( /scripts/build-controller.sh ). github.com/aws-controllers-k8s/$SERVICE-controller \u00b6 Each AWS API that has had a Kubernetes controller built to manage resources in that API has its own source code repository in the github.com/aws-controllers-k8s Github Organization. The source repos will be called $SERVICE-controller . These service controller repositories contain Go code for the main controller binary ( /cmd/controller/ ), the public API types for the controllers ( /apis ), the Go code for the resource managers used by the controller ( /pkg/resource/*/ ), static configuration manifests ( /config ) and Helm charts for the controller installation ( /helm ). API Inference \u00b6 Read about how the code generator infers information about a Kubernetes Custom Resource Definitions (CRDs) from an AWS API model file. Code Generation \u00b6 The code generation section gives you a bit of background on how we go about automating the code generation for controllers and supporting artifacts. Setting up a Development Environemnt \u00b6 In the setup section we walk you through setting up your local Git environment with the repo and how advise you on how we handle contributions. Building an ACK Service Controller \u00b6 After getting your development environment established, you will want to learn how to build an ACK service controller . Testing an ACK Service Controller \u00b6 Last but not least, in the testing section we show you how to test ACK locally.","title":"Overview"},{"location":"dev-docs/overview/#overview","text":"This section of the docs is for ACK contributors.","title":"Overview"},{"location":"dev-docs/overview/#code-organization","text":"ACK is a collection of source repositories containing a common runtime and type system, a code generator and individual service controllers that manage resources in a specific AWS API. github.com/aws-controllers-k8s/community docs and common tests (this repo) github.com/aws-controllers-k8s/runtime : common ACK runtime and types github.com/aws-controllers-k8s/code-generator : the code generator and templates github.com/aws-controllers-k8s/$SERVICE-controller : individual ACK controllers for AWS services.","title":"Code Organization"},{"location":"dev-docs/overview/#githubcomaws-controllers-k8scommunity-this-repo","text":"The github.com/aws-controllers-k8s/community source code repository (this repo) contains the common test scripts and documentation that gets published to https://aws-controllers-k8s.github.io/community/.","title":"github.com/aws-controllers-k8s/community (this repo)"},{"location":"dev-docs/overview/#githubcomaws-controllers-k8sruntime","text":"The github.com/aws-controllers-k8s/runtime source code repository contains the common ACK controller runtime ( /pkg/runtime , /pkg/types ) and core public Kubernetes API types ( /apis/core ).","title":"github.com/aws-controllers-k8s/runtime"},{"location":"dev-docs/overview/#githubcomaws-controllers-k8scode-generator","text":"The github.com/aws-controllers-k8s/code-generator source code repository contains the ack-generate CLI tool ( /cmd/ack-generate ), the Go packages that are used in API inference and code generation ( /pkg/generate , /pkg/model ) and Bash scripts to build an ACK service controller ( /scripts/build-controller.sh ).","title":"github.com/aws-controllers-k8s/code-generator"},{"location":"dev-docs/overview/#githubcomaws-controllers-k8sservice-controller","text":"Each AWS API that has had a Kubernetes controller built to manage resources in that API has its own source code repository in the github.com/aws-controllers-k8s Github Organization. The source repos will be called $SERVICE-controller . These service controller repositories contain Go code for the main controller binary ( /cmd/controller/ ), the public API types for the controllers ( /apis ), the Go code for the resource managers used by the controller ( /pkg/resource/*/ ), static configuration manifests ( /config ) and Helm charts for the controller installation ( /helm ).","title":"github.com/aws-controllers-k8s/$SERVICE-controller"},{"location":"dev-docs/overview/#api-inference","text":"Read about how the code generator infers information about a Kubernetes Custom Resource Definitions (CRDs) from an AWS API model file.","title":"API Inference"},{"location":"dev-docs/overview/#code-generation","text":"The code generation section gives you a bit of background on how we go about automating the code generation for controllers and supporting artifacts.","title":"Code Generation"},{"location":"dev-docs/overview/#setting-up-a-development-environemnt","text":"In the setup section we walk you through setting up your local Git environment with the repo and how advise you on how we handle contributions.","title":"Setting up a Development Environemnt"},{"location":"dev-docs/overview/#building-an-ack-service-controller","text":"After getting your development environment established, you will want to learn how to build an ACK service controller .","title":"Building an ACK Service Controller"},{"location":"dev-docs/overview/#testing-an-ack-service-controller","text":"Last but not least, in the testing section we show you how to test ACK locally.","title":"Testing an ACK Service Controller"},{"location":"dev-docs/release/","text":"Release \u00b6 Here we document the release process for ACK service controllers. Remember that there is no single ACK binary. Rather, when we build a release for ACK, that release is for one or more individual ACK service controllers binaries, each of which are installed separately. This documentation covers the steps involved for officially publishing a ACK service controller's release artifacts. Once ACK service controller changes are tested by the service team and they wish to release latest artifacts, service team only needs to create a new release for service-controller github repository with a semver tag (Ex: v0.0.1). Steps below show how to create a new release with semver tag. For more details on semantic versioning(semver), please read docs/contents/releases.md Once the git repository is tagged with semver, a postsubmit prowjob builds binary docker image for ACK service controller and publish to public ecr repository public.ecr.aws/aws-controllers-k8s/controller . Same prowjob also publishes the Helm charts for the ACK service controller to public ecr repository public.ecr.aws/aws-controllers-k8s/controller . Here is a sample release prowjob . What is a release exactly? \u00b6 A \"release\" is the combination of a Git tag containing a SemVer version tag against this source repository and the collection of artifacts that allow the individual ACK service controllers included in that Git commit to be easily installed via Helm. The Git tag points at a specific Git commit referencing the exact source code that comprises the ACK service controllers in that \"release\". The release artifacts include the following for one or more service controllers: Docker image Helm chart The Docker image is built and pushed with an image tag that indicates the release version for the controller along with the AWS service. For example, assume a release semver tag of v0.1.0 that includes service controllers for S3 and SNS. There would be two Docker images built for this release, one each containing the ACK service controllers for S3 and SNS. The Docker images would have the following image tags: s3-v0.1.0 and sns-v0.1.0 . Note that the full image name would be public.ecr.aws/aws-controllers-k8s/controller:s3-v0.1.0 The Helm chart artifact can be used to install the ACK service controller as a Kubernetes Deployment; the Deployment's Pod image will refer to the exact Docker image tag matching the release tag. Release steps \u00b6 First check out a git branch for your release: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 export RELEASE_VERSION = v0.0.1 git checkout -b release- $RELEASE_VERSION ``` 2 . Build the release artifacts for the controllers you wish to include in the release Run ` scripts/build-controller-release.sh ` for each service. For instance, to build release artifacts for the SNS and S3 controllers I would do : ``` bash for SERVICE in sns s3 ; do ./scripts/build-controller-release.sh $SERVICE $RELEASE_VERSION ; done You can review the release artifacts that were built for each service by looking in the services/$SERVICE/helm directory: tree services/$SERVICE/helm or by doing: git diff Note When you run scripts/build-controller-release.sh for a service, it will overwrite any Helm chart files that had previously been generated in the services/$SERVICE/helm directory with files that refer to the Docker image with an image tag referring to the release you've just built artifacts for. Commit your code and create a pull request: 1 git commit -a -m \"release artifacts for release $RELEASE_VERSION \" Get your pull request reviewed and merged. Upon merging the pull request 1 2 git tag -a $RELEASE_VERSION $( git rev-parse HEAD ) git push upstream main --tags Todo A Github Action should execute the above which will end up associating a Git tag (and therefore a Github release) with the SHA1 commit ID of the source code for the controllers and the release artifacts you built for that release version. git tag operation from last step triggers a postsubmit prowjob which builds binary docker image and then publishes both docker image and Helm chart to public ECR repository. Service team can see the release prowjobs, their status and logs at https://prow.ack.aws.dev/ NOTE: This same postsubmit prowjob also publishes the stable Helm charts, whenever there is a code push on stable git branch. When this prowjob is triggered from stable branch, it does not build a docker image and only publishes the helm artifacts with stable tag. Ex: elasticache-v1-stable","title":"Release"},{"location":"dev-docs/release/#release","text":"Here we document the release process for ACK service controllers. Remember that there is no single ACK binary. Rather, when we build a release for ACK, that release is for one or more individual ACK service controllers binaries, each of which are installed separately. This documentation covers the steps involved for officially publishing a ACK service controller's release artifacts. Once ACK service controller changes are tested by the service team and they wish to release latest artifacts, service team only needs to create a new release for service-controller github repository with a semver tag (Ex: v0.0.1). Steps below show how to create a new release with semver tag. For more details on semantic versioning(semver), please read docs/contents/releases.md Once the git repository is tagged with semver, a postsubmit prowjob builds binary docker image for ACK service controller and publish to public ecr repository public.ecr.aws/aws-controllers-k8s/controller . Same prowjob also publishes the Helm charts for the ACK service controller to public ecr repository public.ecr.aws/aws-controllers-k8s/controller . Here is a sample release prowjob .","title":"Release"},{"location":"dev-docs/release/#what-is-a-release-exactly","text":"A \"release\" is the combination of a Git tag containing a SemVer version tag against this source repository and the collection of artifacts that allow the individual ACK service controllers included in that Git commit to be easily installed via Helm. The Git tag points at a specific Git commit referencing the exact source code that comprises the ACK service controllers in that \"release\". The release artifacts include the following for one or more service controllers: Docker image Helm chart The Docker image is built and pushed with an image tag that indicates the release version for the controller along with the AWS service. For example, assume a release semver tag of v0.1.0 that includes service controllers for S3 and SNS. There would be two Docker images built for this release, one each containing the ACK service controllers for S3 and SNS. The Docker images would have the following image tags: s3-v0.1.0 and sns-v0.1.0 . Note that the full image name would be public.ecr.aws/aws-controllers-k8s/controller:s3-v0.1.0 The Helm chart artifact can be used to install the ACK service controller as a Kubernetes Deployment; the Deployment's Pod image will refer to the exact Docker image tag matching the release tag.","title":"What is a release exactly?"},{"location":"dev-docs/release/#release-steps","text":"First check out a git branch for your release: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 export RELEASE_VERSION = v0.0.1 git checkout -b release- $RELEASE_VERSION ``` 2 . Build the release artifacts for the controllers you wish to include in the release Run ` scripts/build-controller-release.sh ` for each service. For instance, to build release artifacts for the SNS and S3 controllers I would do : ``` bash for SERVICE in sns s3 ; do ./scripts/build-controller-release.sh $SERVICE $RELEASE_VERSION ; done You can review the release artifacts that were built for each service by looking in the services/$SERVICE/helm directory: tree services/$SERVICE/helm or by doing: git diff Note When you run scripts/build-controller-release.sh for a service, it will overwrite any Helm chart files that had previously been generated in the services/$SERVICE/helm directory with files that refer to the Docker image with an image tag referring to the release you've just built artifacts for. Commit your code and create a pull request: 1 git commit -a -m \"release artifacts for release $RELEASE_VERSION \" Get your pull request reviewed and merged. Upon merging the pull request 1 2 git tag -a $RELEASE_VERSION $( git rev-parse HEAD ) git push upstream main --tags Todo A Github Action should execute the above which will end up associating a Git tag (and therefore a Github release) with the SHA1 commit ID of the source code for the controllers and the release artifacts you built for that release version. git tag operation from last step triggers a postsubmit prowjob which builds binary docker image and then publishes both docker image and Helm chart to public ECR repository. Service team can see the release prowjobs, their status and logs at https://prow.ack.aws.dev/ NOTE: This same postsubmit prowjob also publishes the stable Helm charts, whenever there is a code push on stable git branch. When this prowjob is triggered from stable branch, it does not build a docker image and only publishes the helm artifacts with stable tag. Ex: elasticache-v1-stable","title":"Release steps"},{"location":"dev-docs/setup/","text":"Setup \u00b6 We walk you now through the setup to start contributing to the AWS Controller for Kubernetes (ACK) project. No matter if you're contributing code or docs, follow the steps below to set up your development environment. Issue before PR Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy. Prerequisites \u00b6 Please ensure that you have properly installed Go . Go version We recommend to use a Go version of 1.14 or above for development. Fork upstream repositories \u00b6 The first step in setting up your ACK development environment is to fork the upstream ACK source code repositories to your personal Github account. There are three common upstream repositories you should fork first: github.com/aws-controllers-k8s/community contains the common docs and tests github.com/aws-controllers-k8s/runtime is the core ACK runtime and types github.com/aws-controllers-k8s/code-generator is the ACK code generator prefix ACK forked repos with 'ack-' When I fork repositories to my personal Github account, I tend to prefix the repositories with a common string for grouping purposes. For ACK source repositories that I forked from the github.com/aws-controllers-k8s Github Organization, I prefix those repositories with \"ack-\". For example, when I forked the github.com/aws-controllers-k8s/code-generator repository to my github.com/jaypipes personal space on Github, I immediately renamed the forked repo to github.com/jaypipes/ack-code-generator . This makes it easier to quickly filter repositories that are forked from the github.com/aws-controllers-k8s Github Organization. After forking the above common repositories, fork the upstream service controller repositories that you wish to work on or test out. The upstream service controller repositories are in the github.com/aws-controllers-k8s Github Organization and follow a naming schema of $SERVICE_ALIAS-controller . So, if you wanted to work on the S3 service controller, you would fork the github.com/aws-controllers-k8s/s3-controller source repository to your personal Github space. Ensure source code organization directories exist \u00b6 Make sure in your $GOPATH/src that you have directories for the aws-controllers-k8s organization: 1 mkdir -p $GOPATH /src/github.com/aws-controllers-k8s git clone forked repositories and add upstream remote \u00b6 For each of your forked repositories, you will git clone the repository into the appropriate folder in your $GOPATH . Once git clone 'd, you will want to set up a Git remote called \"upstream\" (remember that \"origin\" will be pointing at your forked repository location in your personal Github space). You can use this script to do this for you: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 GITHUB_ID = \"your GH username\" # Set this to \"\" if you did NOT take my advice above in the tip about prefixing # your personal forked ACK repository names with \"ack-\" ACK_REPO_PREFIX = \"ack-\" # Clone all the common ACK repositories... COMMON = \"community runtime code-generator\" for REPO in $COMMON ; do cd $GOPATH /src/github.com/aws-controllers-k8s git clone git@github.com: $GITHUB_ID / $ACK_REPO_PREFIX$REPO $REPO cd $REPO git remote add upstream git@github.com:aws-controllers-k8s/ $REPO git fetch --all done # Now clone all the service controller repositories... # Change this to the list of services you forked service controllers for... SERVICES = \"s3 sns ecr\" for SERVICE in $SERVICES ; do cd $GOPATH /src/github.com/aws-controllers-k8s git clone git@github.com: $GITHUB_ID / $ACK_REPO_PREFIX$SERVICE -controller $SERVICE -controller cd $SERVICE -controller git remote add upstream git@github.com:aws-controllers-k8s/ $SERVICE -controller git fetch --all done Create your local branch \u00b6 Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set BRANCH_NAME=docs-improve and then: 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/main Commit changes \u00b6 Make your changes locally, commit and push using: 1 2 3 git commit -a -m \"improves the docs a lot\" git push origin $BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs Create a pull request \u00b6 Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and there open the pull request as depicted below: We monitor the GitHub repo and try to follow up with comments within a working day. Next Steps \u00b6 After getting familiar with the various ACK source code repositories, now learn how to build an ACK service controller .","title":"Setup"},{"location":"dev-docs/setup/#setup","text":"We walk you now through the setup to start contributing to the AWS Controller for Kubernetes (ACK) project. No matter if you're contributing code or docs, follow the steps below to set up your development environment. Issue before PR Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy.","title":"Setup"},{"location":"dev-docs/setup/#prerequisites","text":"Please ensure that you have properly installed Go . Go version We recommend to use a Go version of 1.14 or above for development.","title":"Prerequisites"},{"location":"dev-docs/setup/#fork-upstream-repositories","text":"The first step in setting up your ACK development environment is to fork the upstream ACK source code repositories to your personal Github account. There are three common upstream repositories you should fork first: github.com/aws-controllers-k8s/community contains the common docs and tests github.com/aws-controllers-k8s/runtime is the core ACK runtime and types github.com/aws-controllers-k8s/code-generator is the ACK code generator prefix ACK forked repos with 'ack-' When I fork repositories to my personal Github account, I tend to prefix the repositories with a common string for grouping purposes. For ACK source repositories that I forked from the github.com/aws-controllers-k8s Github Organization, I prefix those repositories with \"ack-\". For example, when I forked the github.com/aws-controllers-k8s/code-generator repository to my github.com/jaypipes personal space on Github, I immediately renamed the forked repo to github.com/jaypipes/ack-code-generator . This makes it easier to quickly filter repositories that are forked from the github.com/aws-controllers-k8s Github Organization. After forking the above common repositories, fork the upstream service controller repositories that you wish to work on or test out. The upstream service controller repositories are in the github.com/aws-controllers-k8s Github Organization and follow a naming schema of $SERVICE_ALIAS-controller . So, if you wanted to work on the S3 service controller, you would fork the github.com/aws-controllers-k8s/s3-controller source repository to your personal Github space.","title":"Fork upstream repositories"},{"location":"dev-docs/setup/#ensure-source-code-organization-directories-exist","text":"Make sure in your $GOPATH/src that you have directories for the aws-controllers-k8s organization: 1 mkdir -p $GOPATH /src/github.com/aws-controllers-k8s","title":"Ensure source code organization directories exist"},{"location":"dev-docs/setup/#git-clone-forked-repositories-and-add-upstream-remote","text":"For each of your forked repositories, you will git clone the repository into the appropriate folder in your $GOPATH . Once git clone 'd, you will want to set up a Git remote called \"upstream\" (remember that \"origin\" will be pointing at your forked repository location in your personal Github space). You can use this script to do this for you: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 GITHUB_ID = \"your GH username\" # Set this to \"\" if you did NOT take my advice above in the tip about prefixing # your personal forked ACK repository names with \"ack-\" ACK_REPO_PREFIX = \"ack-\" # Clone all the common ACK repositories... COMMON = \"community runtime code-generator\" for REPO in $COMMON ; do cd $GOPATH /src/github.com/aws-controllers-k8s git clone git@github.com: $GITHUB_ID / $ACK_REPO_PREFIX$REPO $REPO cd $REPO git remote add upstream git@github.com:aws-controllers-k8s/ $REPO git fetch --all done # Now clone all the service controller repositories... # Change this to the list of services you forked service controllers for... SERVICES = \"s3 sns ecr\" for SERVICE in $SERVICES ; do cd $GOPATH /src/github.com/aws-controllers-k8s git clone git@github.com: $GITHUB_ID / $ACK_REPO_PREFIX$SERVICE -controller $SERVICE -controller cd $SERVICE -controller git remote add upstream git@github.com:aws-controllers-k8s/ $SERVICE -controller git fetch --all done","title":"git clone forked repositories and add upstream remote"},{"location":"dev-docs/setup/#create-your-local-branch","text":"Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set BRANCH_NAME=docs-improve and then: 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/main","title":"Create your local branch"},{"location":"dev-docs/setup/#commit-changes","text":"Make your changes locally, commit and push using: 1 2 3 git commit -a -m \"improves the docs a lot\" git push origin $BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs","title":"Commit changes"},{"location":"dev-docs/setup/#create-a-pull-request","text":"Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and there open the pull request as depicted below: We monitor the GitHub repo and try to follow up with comments within a working day.","title":"Create a pull request"},{"location":"dev-docs/setup/#next-steps","text":"After getting familiar with the various ACK source code repositories, now learn how to build an ACK service controller .","title":"Next Steps"},{"location":"dev-docs/testing/","text":"Testing \u00b6 In the following, we will take you through the steps to run end-to-end (e2e) tests for the ACK service controller for S3. You may use these steps to run e2e tests for other ACK service controllers. If you run into any problems when testing a service controller, please raise an issue with the details so we can reproduce your issue. Prerequisites \u00b6 For local development and testing we use \"Kubernetes in Docker\" ( kind ), which in turn requires Docker. Footprint When you run the scripts/kind-build-test.sh script the first time, the step that builds the container image for the target ACK service controller can take up to 40 or more minutes. This is because the container image contains a lot of dependencies. Once you successfully build the target image this base image layer is cached locally, and the build takes a much shorter amount of time. We are aware of this (and the storage footprint, ca. 3 GB) and aim to reduce both in the fullness of time. In summary, in order to test ACK you will need to have the following tools installed and configured: Golang 1.14+ make Docker kind jq To build and test an ACK controller with kind , execute the commands as described in the following from the root directory of the github.com/aws-controllers-k8s/community repository. You should have forked this repository and git clone 'd it locally when setting up your development environment . Recommended RAM Given that our test setup creates the container images and then launches a test cluster, we recommend that you have at least 4GB of RAM available for the tests. With the prerequisites out of the way, let's move on to running e2e tests for a service controller. Run tests \u00b6 Time to run the end-to-end test. IAM setup \u00b6 In order for the ACK service controller to manage the S3 bucket, it needs an identity. In other words, it needs an IAM role that represents the ACK service controller towards the S3 service. First, define the name of the IAM role that will have the permission to manage S3 buckets on your behalf: 1 export ACK_TEST_IAM_ROLE = Admin - k8s Now we need to verify the IAM principal (likely an IAM user) that is going to assume the IAM role ACK_TEST_IAM_ROLE . So to get its ARN, execute: 1 export ACK_TEST_PRINCIPAL_ARN =$ ( aws sts get - caller - identity -- query 'Arn' -- output text ) You can verify if that worked using echo $ACK_TEST_PRINCIPAL_ARN and that should print something along the lines of arn:aws:iam::1234567890121:user/ausername . Next up, create the IAM role, adding the necessary trust relationship to the role, using the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 cat > trust-policy.json << EOF { \"Version\": \"2012-10-17\", \"Statement\": { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"$ACK_TEST_PRINCIPAL_ARN\" }, \"Action\": \"sts:AssumeRole\" } } EOF Using above trust policy, we can now create the IAM role: 1 2 3 aws iam create-role \\ --role-name $ACK_TEST_IAM_ROLE \\ --assume-role-policy-document file://trust-policy.json Now we're in the position to give the IAM role ACK_TEST_IAM_ROLE the permission to handle S3 buckets for us, using: 1 2 3 aws iam attach-role-policy \\ --role-name $ACK_TEST_IAM_ROLE \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonS3FullAccess\" Access delegation in IAM If you're not that familiar with IAM access delegation, we recommend you to peruse the IAM documentation Next, in order for our test to generate temporary credentials we need to tell it to use the IAM role we created in the previous step. To generate the IAM role ARN, do: 1 2 AWS_ACCOUNT_ID =$ ( aws sts get - caller - identity -- query 'Account' -- output text ) && \\ export ACK_ROLE_ARN = arn : aws : iam :: $ { AWS_ACCOUNT_ID }: role /$ { ACK_TEST_IAM_ROLE } Info The tests uses the generate_temp_creds function from the scripts/lib/aws.sh script, executing effectively aws sts assume-role --role-session-arn $ACK_ROLE_ARN --role-session-name $TEMP_ROLE which fetches temporarily AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and an AWS_SESSION_TOKEN used in turn to authentication the ACK controller. The duration of the session token is 900 seconds (15 minutes). Phew that was a lot to set up, but good news: you're almost there. Run end-to-end test \u00b6 Before you proceed, make sure that you've done the IAM setup in the previous step. IAM troubles?! If you try the following command and you see an error message containing something along the line of ACK_ROLE_ARN is not defined. then you know that somewhere in the IAM setup you either left out a step or one of the commands failed. Now we're finally in the position to execute the end-to-end test: 1 make kind-test SERVICE=$SERVICE This provisions a Kubernetes cluster using kind , builds a container image with the ACK service controller, and loads the container image into the kind cluster. It then installs the ACK service controller and related Kubernetes manifests into the kind cluster using kustomize build | kubectl apply -f - . Then, the above script runs a series of test scripts that call kubectl and the aws CLI tools to verify that custom resources of the type managed by the respective ACK service controller is created, updated and deleted appropriately (still TODO). Finally, it will run tests that create resources for the respective service and verify if the resource has successfully created. In our example case it should create an S3 bucket and then destroy it again, yielding something like the following (edited down to the relevant parts): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ... ./ scripts / kind - build - test . sh - s s3 Using Kubernetes kindest / node : v1 . 16.9 @ sha256 : 7175872357 bc85847ec4b1aba46ed1d12fa054c83ac7a8a11f5c268957fd5765 Creating k8s cluster using \"kind\" ... No kind clusters found . Created k8s cluster using \"kind\" Building s3 docker image Building 's3' controller docker image with tag : ack - s3 - controller : ec452ed sha256 : c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef Loading the images into the cluster Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-worker\" , loading ... Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-control-plane\" , loading ... Loading CRD manifests for s3 into the cluster customresourcedefinition . apiextensions . k8s . io / buckets . s3 . services . k8s . aws created Loading RBAC manifests for s3 into the cluster clusterrole . rbac . authorization . k8s . io / ack - controller - role created clusterrolebinding . rbac . authorization . k8s . io / ack - controller - rolebinding created Loading service controller Deployment for s3 into the cluster 2020 / 08 / 18 09 : 51 : 46 Fixed the missing field by adding apiVersion : kustomize . config . k8s . io / v1beta1 Fixed the missing field by adding kind : Kustomization namespace / ack - system created deployment . apps / ack - s3 - controller created Running aws sts assume - role -- role - arn arn : aws : iam :: 1234567890121 : role / Admin - k8s , -- role - session - name tmp - role - 1 b779de5 -- duration - seconds 900 , Temporary credentials generated deployment . apps / ack - s3 - controller env updated Added AWS Credentials to env vars map ====================================================================================================== To poke around your test manually : export KUBECONFIG =/ Users / hausenbl / ACK / upstream / aws - controllers - k8s / scripts /../ build / tmp - test - ccc3c7f1 / kubeconfig kubectl get pods - A ====================================================================================================== bucket . s3 . services . k8s . aws / ack - test - smoke - s3 created { \"Name\" : \"ack-test-smoke-s3\" , \"CreationDate\" : \"2020-08-18T08:52:04+00:00\" } bucket . s3 . services . k8s . aws \"ack-test-smoke-s3\" deleted smoke took 27 second ( s ) \ud83e\udd51 Deleting k8s cluster using \"kind\" Deleting cluster \"test-ccc3c7f1\" ... As you can see, in above case the end-to-end test (creating cluster, deploying ACK, applying custom resources, and tear-down) took less than 30 seconds. This is for the warmed caches case. Repeat for other services \u00b6 We have end-to-end tests for all services listed in the DEVELOPER-PREVIEW , BETA and GA release statuses in our service listing document. Simply replace your SERVICE environment variable with the name of a supported service and re-run the IAM and test steps outlined above. Background \u00b6 We use mockery for unit testing. You can install it by following the guideline on mockery's GitHub or simply by running our handy script at ./scripts/install-mockery.sh for general Linux environments. We track testing in the umbrella issue 6 . on GitHub. Use this issue as a starting point and if you create a new testing-related issue, mention it from there. Clean up \u00b6 To clean up a kind cluster, including the container images and configuration files created by the script specifically for said test cluster, execute: 1 kind delete cluster --name $CLUSTER_NAME If you want to delete all kind cluster running on your machine, use: 1 make delete-all-kind-clusters With this the testing is completed. Thanks for your time and we appreciate your feedback.","title":"Testing"},{"location":"dev-docs/testing/#testing","text":"In the following, we will take you through the steps to run end-to-end (e2e) tests for the ACK service controller for S3. You may use these steps to run e2e tests for other ACK service controllers. If you run into any problems when testing a service controller, please raise an issue with the details so we can reproduce your issue.","title":"Testing"},{"location":"dev-docs/testing/#prerequisites","text":"For local development and testing we use \"Kubernetes in Docker\" ( kind ), which in turn requires Docker. Footprint When you run the scripts/kind-build-test.sh script the first time, the step that builds the container image for the target ACK service controller can take up to 40 or more minutes. This is because the container image contains a lot of dependencies. Once you successfully build the target image this base image layer is cached locally, and the build takes a much shorter amount of time. We are aware of this (and the storage footprint, ca. 3 GB) and aim to reduce both in the fullness of time. In summary, in order to test ACK you will need to have the following tools installed and configured: Golang 1.14+ make Docker kind jq To build and test an ACK controller with kind , execute the commands as described in the following from the root directory of the github.com/aws-controllers-k8s/community repository. You should have forked this repository and git clone 'd it locally when setting up your development environment . Recommended RAM Given that our test setup creates the container images and then launches a test cluster, we recommend that you have at least 4GB of RAM available for the tests. With the prerequisites out of the way, let's move on to running e2e tests for a service controller.","title":"Prerequisites"},{"location":"dev-docs/testing/#run-tests","text":"Time to run the end-to-end test.","title":"Run tests"},{"location":"dev-docs/testing/#iam-setup","text":"In order for the ACK service controller to manage the S3 bucket, it needs an identity. In other words, it needs an IAM role that represents the ACK service controller towards the S3 service. First, define the name of the IAM role that will have the permission to manage S3 buckets on your behalf: 1 export ACK_TEST_IAM_ROLE = Admin - k8s Now we need to verify the IAM principal (likely an IAM user) that is going to assume the IAM role ACK_TEST_IAM_ROLE . So to get its ARN, execute: 1 export ACK_TEST_PRINCIPAL_ARN =$ ( aws sts get - caller - identity -- query 'Arn' -- output text ) You can verify if that worked using echo $ACK_TEST_PRINCIPAL_ARN and that should print something along the lines of arn:aws:iam::1234567890121:user/ausername . Next up, create the IAM role, adding the necessary trust relationship to the role, using the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 cat > trust-policy.json << EOF { \"Version\": \"2012-10-17\", \"Statement\": { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"$ACK_TEST_PRINCIPAL_ARN\" }, \"Action\": \"sts:AssumeRole\" } } EOF Using above trust policy, we can now create the IAM role: 1 2 3 aws iam create-role \\ --role-name $ACK_TEST_IAM_ROLE \\ --assume-role-policy-document file://trust-policy.json Now we're in the position to give the IAM role ACK_TEST_IAM_ROLE the permission to handle S3 buckets for us, using: 1 2 3 aws iam attach-role-policy \\ --role-name $ACK_TEST_IAM_ROLE \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonS3FullAccess\" Access delegation in IAM If you're not that familiar with IAM access delegation, we recommend you to peruse the IAM documentation Next, in order for our test to generate temporary credentials we need to tell it to use the IAM role we created in the previous step. To generate the IAM role ARN, do: 1 2 AWS_ACCOUNT_ID =$ ( aws sts get - caller - identity -- query 'Account' -- output text ) && \\ export ACK_ROLE_ARN = arn : aws : iam :: $ { AWS_ACCOUNT_ID }: role /$ { ACK_TEST_IAM_ROLE } Info The tests uses the generate_temp_creds function from the scripts/lib/aws.sh script, executing effectively aws sts assume-role --role-session-arn $ACK_ROLE_ARN --role-session-name $TEMP_ROLE which fetches temporarily AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and an AWS_SESSION_TOKEN used in turn to authentication the ACK controller. The duration of the session token is 900 seconds (15 minutes). Phew that was a lot to set up, but good news: you're almost there.","title":"IAM setup"},{"location":"dev-docs/testing/#run-end-to-end-test","text":"Before you proceed, make sure that you've done the IAM setup in the previous step. IAM troubles?! If you try the following command and you see an error message containing something along the line of ACK_ROLE_ARN is not defined. then you know that somewhere in the IAM setup you either left out a step or one of the commands failed. Now we're finally in the position to execute the end-to-end test: 1 make kind-test SERVICE=$SERVICE This provisions a Kubernetes cluster using kind , builds a container image with the ACK service controller, and loads the container image into the kind cluster. It then installs the ACK service controller and related Kubernetes manifests into the kind cluster using kustomize build | kubectl apply -f - . Then, the above script runs a series of test scripts that call kubectl and the aws CLI tools to verify that custom resources of the type managed by the respective ACK service controller is created, updated and deleted appropriately (still TODO). Finally, it will run tests that create resources for the respective service and verify if the resource has successfully created. In our example case it should create an S3 bucket and then destroy it again, yielding something like the following (edited down to the relevant parts): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ... ./ scripts / kind - build - test . sh - s s3 Using Kubernetes kindest / node : v1 . 16.9 @ sha256 : 7175872357 bc85847ec4b1aba46ed1d12fa054c83ac7a8a11f5c268957fd5765 Creating k8s cluster using \"kind\" ... No kind clusters found . Created k8s cluster using \"kind\" Building s3 docker image Building 's3' controller docker image with tag : ack - s3 - controller : ec452ed sha256 : c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef Loading the images into the cluster Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-worker\" , loading ... Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-control-plane\" , loading ... Loading CRD manifests for s3 into the cluster customresourcedefinition . apiextensions . k8s . io / buckets . s3 . services . k8s . aws created Loading RBAC manifests for s3 into the cluster clusterrole . rbac . authorization . k8s . io / ack - controller - role created clusterrolebinding . rbac . authorization . k8s . io / ack - controller - rolebinding created Loading service controller Deployment for s3 into the cluster 2020 / 08 / 18 09 : 51 : 46 Fixed the missing field by adding apiVersion : kustomize . config . k8s . io / v1beta1 Fixed the missing field by adding kind : Kustomization namespace / ack - system created deployment . apps / ack - s3 - controller created Running aws sts assume - role -- role - arn arn : aws : iam :: 1234567890121 : role / Admin - k8s , -- role - session - name tmp - role - 1 b779de5 -- duration - seconds 900 , Temporary credentials generated deployment . apps / ack - s3 - controller env updated Added AWS Credentials to env vars map ====================================================================================================== To poke around your test manually : export KUBECONFIG =/ Users / hausenbl / ACK / upstream / aws - controllers - k8s / scripts /../ build / tmp - test - ccc3c7f1 / kubeconfig kubectl get pods - A ====================================================================================================== bucket . s3 . services . k8s . aws / ack - test - smoke - s3 created { \"Name\" : \"ack-test-smoke-s3\" , \"CreationDate\" : \"2020-08-18T08:52:04+00:00\" } bucket . s3 . services . k8s . aws \"ack-test-smoke-s3\" deleted smoke took 27 second ( s ) \ud83e\udd51 Deleting k8s cluster using \"kind\" Deleting cluster \"test-ccc3c7f1\" ... As you can see, in above case the end-to-end test (creating cluster, deploying ACK, applying custom resources, and tear-down) took less than 30 seconds. This is for the warmed caches case.","title":"Run end-to-end test"},{"location":"dev-docs/testing/#repeat-for-other-services","text":"We have end-to-end tests for all services listed in the DEVELOPER-PREVIEW , BETA and GA release statuses in our service listing document. Simply replace your SERVICE environment variable with the name of a supported service and re-run the IAM and test steps outlined above.","title":"Repeat for other services"},{"location":"dev-docs/testing/#background","text":"We use mockery for unit testing. You can install it by following the guideline on mockery's GitHub or simply by running our handy script at ./scripts/install-mockery.sh for general Linux environments. We track testing in the umbrella issue 6 . on GitHub. Use this issue as a starting point and if you create a new testing-related issue, mention it from there.","title":"Background"},{"location":"dev-docs/testing/#clean-up","text":"To clean up a kind cluster, including the container images and configuration files created by the script specifically for said test cluster, execute: 1 kind delete cluster --name $CLUSTER_NAME If you want to delete all kind cluster running on your machine, use: 1 make delete-all-kind-clusters With this the testing is completed. Thanks for your time and we appreciate your feedback.","title":"Clean up"},{"location":"user-docs/authorization/","text":"Authorization \u00b6 When we talk about authorization and access control for ACK, we need to discuss two different Role-based Access Control (RBAC) systems. Remember that Kubernetes RBAC governs a Kubernetes user's ability to read or write Kubernetes resources . In the case of ACK, this means that Kubernetes RBAC system controls the ability of a Kubernetes user to read or write different custom resources (CRs) that ACK service controllers use. On the other end of the authorization spectrum, you can use AWS Identity and Access Management (IAM) Policies to governs the ability of an AWS IAM Role to read or write certain AWS resources . IAM is more than RBAC AWS IAM is more than just an RBAC system. It handles authentication/identification and can be used to build Attribute-based Access Control (ABAC) systems. In this document, however, we're focusing on using IAM primitives to establish an RBAC system. These two RBAC systems do not overlap . The Kubernetes user that calls the Kubernetes API via calls to kubectl has no association with an IAM Role . Instead, it is the ServiceAccount running the ACK service controller's Pod that is associated with an IAM Role and is thus governed by the IAM RBAC system. RBAC authorization mode The above diagram assumes you are running Kubernetes API server with the RBAC authorization mode enabled. Configure permissions \u00b6 Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions. Configuring Kubernetes RBAC \u00b6 As part of installation, certain Kubernetes Role resources will be created that contain permissions to modify the Kubernetes custom resources (CRs) that the ACK service controller is responsible for. Important All Kubernetes CRs managed by an ACK service controller are Namespaced resources; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role resources are created when installing an ACK service controller: ack-$SERVICE-writer : a Role used for reading and mutating namespace-scoped custom resources that the service controller manages. ack-$SERVICE-reader : a Role used for reading namespaced-scoped custom resources that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRDs and CRs associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack-s3-writerr Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack-s3-reader Role would have been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws/Bucket . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack-sns-writer Role to read/write CRs of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack-sns-reader Role to read CRs of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Bind a Kubernetes User to a Kubernetes Role \u00b6 Once the Kubernetes Role resources have been created, you will want to assign a specific Kubernetes User to a particular Role . You do this using standard Kubernetes RoleBinding resource. For example, assume you want to have the Kubernetes User named \"alice\" have the ability to create, read, delete and modify S3 Buckets in the \"testing\" Kubernetes Namespace and the ability to just read SNS Topic CRs in the Kubernetes \"production\" Namespace you would execute the following commands: 1 2 kubectl create rolebinding alice-ack-s3-writer --role ack-s3-writer --namespace testing --user alice kubectl create rolebinding alice-ack-sns--reader --role ack-sns-reader --namespace production --user alice As always, if you are curious whether a particular Kubernetes user can perform some action on a Kubernetes resource, you can use the kubectl auth can-i command, like this example shows: 1 kubectl auth can-i create buckets --namespace default Configuring AWS IAM \u00b6 Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. The IAM Role that your ACK service controller runs as will need a different set of IAM Policies depending on which AWS service API the service controller is managing. For instance, the ACK service controller for S3 will need permissions to read and write S3 Buckets. We include with each service controller a recommended IAM Policy that restricts the ACK service controller to taking only the actions that the IAM Role needs to properly manage resources for that specific AWS service API. Within each service controller's source code directory is a config/iam/recommended-policy-arn document that contains the AWS Resource Name (ARN) of the recommended managed policy for that service and can be applied to the IAM Role for the ACK service controller by calling aws iam attach-role-policy on the contents of that file: 1 2 3 4 5 6 7 SERVICE = s3 BASE_URL = https://github.com/aws/aws-controllers-k8s/blob/main/services POLICY_URL = $BASE_URL / $SERVICE /config/iam/recommended-policy-arn POLICY_ARN = \"`wget -qO- $POLICY_URL `\" aws iam attach-role-policy \\ --role-name $IAM_ROLE \\ --policy-arn $POLICY_ARN Note Set the $IAM_ROLE variable above to the ARN of the IAM Role the ACK service controller will run as. Some services may need an additional inline policy, specified in config/iam/additional-policy , in addition to the managed policy from recommended-policy-arn . For example, the service controller may require iam:PassRole permission in order to pass an execution role which will be assumed by the AWS service. With $IAM_ROLE still set, run the script at config/iam/additional-policy if there is one to create the inline policy. Cross-account resource management \u00b6 ACK service controllers can manage resources in different AWS accounts. To enable and start using this feature, as an administrator, you will need to: 1. Configure your AWS accounts, where the resources will be managed. 2. Create a ConfigMap to map AWS accounts with the Role ARNs that needs to be assumed 3. Annotate namespaces with AWS Account IDs For detailed information about how ACK service controllers manage resource in multiple AWS accounts, please refer to CARM design document. Setting up AWS accounts \u00b6 AWS Account administrators should create/configure IAM roles to allow ACK service controllers to assume Roles in different AWS accounts. For example, to allow account A (000000000000) to create s3 buckets in account B (111111111111) you can use the following commands 1 2 3 4 5 # Using account B credentials aws iam create-role --role-name s3FullAccess \\ --assume-role-policy-document '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": \"arn:aws:iam::000000000000:role/roleA-production\"}, \"Action\": \"sts:AssumeRole\"}]}' aws iam attach-role-policy --role-name s3FullAccess \\ --policy-arn 'arn:aws:iam::aws:policy/service-role/AmazonS3FullAccess' Map AWS Accounts with their associated Role ARNs \u00b6 After you will need to create a ConfigMap to associate each AWS Account ID with the role ARN that needs be assumed, in order to manage resources in that particular account. 1 2 3 4 5 6 7 8 9 cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: ack-role-account-map namespace: ack-system data: \"111111111111\": arn:aws:iam::111111111111:role/s3FullAccess EOF Bind accounts to namespaces \u00b6 To bind AWS accounts to a specific Namespace you will have to annotate the Namespace with an AWS Account ID. For example: 1 2 3 4 5 6 7 8 cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: production annotations: services.k8s.aws/owner-account-id: 111111111111 EOF For existing namespaces you can also run: 1 kubectl annotate namespace production services.k8s.aws/owner-account-id = 111111111111 Create resource in different AWS accounts \u00b6 Now to create resources in account B you will have to create your CRs in the associated Namespace . For example to create an s3 bucket in account B you can run the following command: 1 2 3 4 5 6 7 8 9 cat <<EOF | kubectl apply -f - apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: my-bucket namespace: production spec: name: my-bucket EOF","title":"Authorization"},{"location":"user-docs/authorization/#authorization","text":"When we talk about authorization and access control for ACK, we need to discuss two different Role-based Access Control (RBAC) systems. Remember that Kubernetes RBAC governs a Kubernetes user's ability to read or write Kubernetes resources . In the case of ACK, this means that Kubernetes RBAC system controls the ability of a Kubernetes user to read or write different custom resources (CRs) that ACK service controllers use. On the other end of the authorization spectrum, you can use AWS Identity and Access Management (IAM) Policies to governs the ability of an AWS IAM Role to read or write certain AWS resources . IAM is more than RBAC AWS IAM is more than just an RBAC system. It handles authentication/identification and can be used to build Attribute-based Access Control (ABAC) systems. In this document, however, we're focusing on using IAM primitives to establish an RBAC system. These two RBAC systems do not overlap . The Kubernetes user that calls the Kubernetes API via calls to kubectl has no association with an IAM Role . Instead, it is the ServiceAccount running the ACK service controller's Pod that is associated with an IAM Role and is thus governed by the IAM RBAC system. RBAC authorization mode The above diagram assumes you are running Kubernetes API server with the RBAC authorization mode enabled.","title":"Authorization"},{"location":"user-docs/authorization/#configure-permissions","text":"Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions.","title":"Configure permissions"},{"location":"user-docs/authorization/#configuring-kubernetes-rbac","text":"As part of installation, certain Kubernetes Role resources will be created that contain permissions to modify the Kubernetes custom resources (CRs) that the ACK service controller is responsible for. Important All Kubernetes CRs managed by an ACK service controller are Namespaced resources; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role resources are created when installing an ACK service controller: ack-$SERVICE-writer : a Role used for reading and mutating namespace-scoped custom resources that the service controller manages. ack-$SERVICE-reader : a Role used for reading namespaced-scoped custom resources that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRDs and CRs associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack-s3-writerr Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack-s3-reader Role would have been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws/Bucket . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack-sns-writer Role to read/write CRs of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack-sns-reader Role to read CRs of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process.","title":"Configuring Kubernetes RBAC"},{"location":"user-docs/authorization/#bind-a-kubernetes-user-to-a-kubernetes-role","text":"Once the Kubernetes Role resources have been created, you will want to assign a specific Kubernetes User to a particular Role . You do this using standard Kubernetes RoleBinding resource. For example, assume you want to have the Kubernetes User named \"alice\" have the ability to create, read, delete and modify S3 Buckets in the \"testing\" Kubernetes Namespace and the ability to just read SNS Topic CRs in the Kubernetes \"production\" Namespace you would execute the following commands: 1 2 kubectl create rolebinding alice-ack-s3-writer --role ack-s3-writer --namespace testing --user alice kubectl create rolebinding alice-ack-sns--reader --role ack-sns-reader --namespace production --user alice As always, if you are curious whether a particular Kubernetes user can perform some action on a Kubernetes resource, you can use the kubectl auth can-i command, like this example shows: 1 kubectl auth can-i create buckets --namespace default","title":"Bind a Kubernetes User to a Kubernetes Role"},{"location":"user-docs/authorization/#configuring-aws-iam","text":"Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. The IAM Role that your ACK service controller runs as will need a different set of IAM Policies depending on which AWS service API the service controller is managing. For instance, the ACK service controller for S3 will need permissions to read and write S3 Buckets. We include with each service controller a recommended IAM Policy that restricts the ACK service controller to taking only the actions that the IAM Role needs to properly manage resources for that specific AWS service API. Within each service controller's source code directory is a config/iam/recommended-policy-arn document that contains the AWS Resource Name (ARN) of the recommended managed policy for that service and can be applied to the IAM Role for the ACK service controller by calling aws iam attach-role-policy on the contents of that file: 1 2 3 4 5 6 7 SERVICE = s3 BASE_URL = https://github.com/aws/aws-controllers-k8s/blob/main/services POLICY_URL = $BASE_URL / $SERVICE /config/iam/recommended-policy-arn POLICY_ARN = \"`wget -qO- $POLICY_URL `\" aws iam attach-role-policy \\ --role-name $IAM_ROLE \\ --policy-arn $POLICY_ARN Note Set the $IAM_ROLE variable above to the ARN of the IAM Role the ACK service controller will run as. Some services may need an additional inline policy, specified in config/iam/additional-policy , in addition to the managed policy from recommended-policy-arn . For example, the service controller may require iam:PassRole permission in order to pass an execution role which will be assumed by the AWS service. With $IAM_ROLE still set, run the script at config/iam/additional-policy if there is one to create the inline policy.","title":"Configuring AWS IAM"},{"location":"user-docs/authorization/#cross-account-resource-management","text":"ACK service controllers can manage resources in different AWS accounts. To enable and start using this feature, as an administrator, you will need to: 1. Configure your AWS accounts, where the resources will be managed. 2. Create a ConfigMap to map AWS accounts with the Role ARNs that needs to be assumed 3. Annotate namespaces with AWS Account IDs For detailed information about how ACK service controllers manage resource in multiple AWS accounts, please refer to CARM design document.","title":"Cross-account resource management"},{"location":"user-docs/authorization/#setting-up-aws-accounts","text":"AWS Account administrators should create/configure IAM roles to allow ACK service controllers to assume Roles in different AWS accounts. For example, to allow account A (000000000000) to create s3 buckets in account B (111111111111) you can use the following commands 1 2 3 4 5 # Using account B credentials aws iam create-role --role-name s3FullAccess \\ --assume-role-policy-document '{\"Version\": \"2012-10-17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"AWS\": \"arn:aws:iam::000000000000:role/roleA-production\"}, \"Action\": \"sts:AssumeRole\"}]}' aws iam attach-role-policy --role-name s3FullAccess \\ --policy-arn 'arn:aws:iam::aws:policy/service-role/AmazonS3FullAccess'","title":"Setting up AWS accounts"},{"location":"user-docs/authorization/#map-aws-accounts-with-their-associated-role-arns","text":"After you will need to create a ConfigMap to associate each AWS Account ID with the role ARN that needs be assumed, in order to manage resources in that particular account. 1 2 3 4 5 6 7 8 9 cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: ack-role-account-map namespace: ack-system data: \"111111111111\": arn:aws:iam::111111111111:role/s3FullAccess EOF","title":"Map AWS Accounts with their associated Role ARNs"},{"location":"user-docs/authorization/#bind-accounts-to-namespaces","text":"To bind AWS accounts to a specific Namespace you will have to annotate the Namespace with an AWS Account ID. For example: 1 2 3 4 5 6 7 8 cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: production annotations: services.k8s.aws/owner-account-id: 111111111111 EOF For existing namespaces you can also run: 1 kubectl annotate namespace production services.k8s.aws/owner-account-id = 111111111111","title":"Bind accounts to namespaces"},{"location":"user-docs/authorization/#create-resource-in-different-aws-accounts","text":"Now to create resources in account B you will have to create your CRs in the associated Namespace . For example to create an s3 bucket in account B you can run the following command: 1 2 3 4 5 6 7 8 9 cat <<EOF | kubectl apply -f - apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: my-bucket namespace: production spec: name: my-bucket EOF","title":"Create resource in different AWS accounts"},{"location":"user-docs/install/","text":"Install \u00b6 In the following we walk you through installing an ACK service controller. Docker images \u00b6 No single ACK Docker image Note that there is no single ACK Docker image. Instead, there are Docker images for each individual ACK service controller that manages resources for a particular AWS API. Each ACK service controller is packaged into a separate container image, published on a public ECR repository . Individual ACK service controllers are tagged with $SERVICE-$VERSION Docker image tags, allowing you to download/test specific ACK service controllers. For example, if you wanted to test the v0.1.0 release image of the ACK service controller for S3, you would pull the public.ecr.aws/aws-controllers-k8s/controller:s3-v0.1.0 image. No 'latest' tag It is not good practice to rely on a :latest default image tag. There are actually no images tagged with a :latest tag in our image repositories. You should always specify a $SERVICE-$VERSION tag when referencing an ACK service controller image. Helm (recommended) \u00b6 The recommended way to install an ACK service controller for Kubernetes is to use Helm 3. Please ensure you have installed Helm 3 to your local environment before running these steps. Each ACK service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the ACK service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. To view the Helm charts available for installation, check the ECR public repository for the ACK Helm charts . Click on the \"Image tags\" tab and take a note of the Helm chart tag for the service controller and version you wish to install. Before installing a Helm chart, you must first make the Helm chart available on the deployment host. To do so, use the helm chart pull and helm chart export commands: 1 2 3 4 5 6 7 8 9 10 11 export HELM_EXPERIMENTAL_OCI = 1 export SERVICE = s3 export RELEASE_VERSION = v0.0.1 export CHART_EXPORT_PATH = /tmp/chart export CHART_REPO = public.ecr.aws/aws-controllers-k8s/chart export CHART_REF = $CHART_REPO : $SERVICE - $RELEASE_VERSION mkdir -p $CHART_EXPORT_PATH helm chart pull $CHART_REF helm chart export $CHART_REF --destination $CHART_EXPORT_PATH You then install a particular ACK service controller using the helm install CLI command: 1 2 3 4 5 6 export ACK_K8S_NAMESPACE = ack-system kubectl create namespace $ACK_K8S_NAMESPACE helm install --namespace $ACK_K8S_NAMESPACE ack- $SERVICE -controller \\ $CHART_EXPORT_PATH /ack- $SERVICE -controller You will see the Helm chart installed: 1 2 3 4 5 6 7 $ helm install --namespace $ACK_K8S_NAMESPACE ack- $SERVICE -controller $CHART_EXPORT_PATH /ack- $SERVICE -controller NAME: ack-s3-controller LAST DEPLOYED: Thu Dec 17 13 :09:17 2020 NAMESPACE: ack-system STATUS: deployed REVISION: 1 TEST SUITE: None You may then verify the Helm chart was installed using the helm list command: 1 helm list --namespace $ACK_K8S_NAMESPACE -o yaml you should see your newly-deployed Helm chart release: 1 2 3 4 5 6 7 8 $ helm list --namespace $ACK_K8S_NAMESPACE -o yaml - app_version: v0.0.1 chart: ack-s3-controller-v0.0.1 name: ack-s3-controller namespace: ack-system revision: \"1\" status: deployed updated: 2020 -12-17 13 :09:17.309002201 -0500 EST Static Kubernetes manifests \u00b6 If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. TODO(jaypipes)","title":"Install"},{"location":"user-docs/install/#install","text":"In the following we walk you through installing an ACK service controller.","title":"Install"},{"location":"user-docs/install/#docker-images","text":"No single ACK Docker image Note that there is no single ACK Docker image. Instead, there are Docker images for each individual ACK service controller that manages resources for a particular AWS API. Each ACK service controller is packaged into a separate container image, published on a public ECR repository . Individual ACK service controllers are tagged with $SERVICE-$VERSION Docker image tags, allowing you to download/test specific ACK service controllers. For example, if you wanted to test the v0.1.0 release image of the ACK service controller for S3, you would pull the public.ecr.aws/aws-controllers-k8s/controller:s3-v0.1.0 image. No 'latest' tag It is not good practice to rely on a :latest default image tag. There are actually no images tagged with a :latest tag in our image repositories. You should always specify a $SERVICE-$VERSION tag when referencing an ACK service controller image.","title":"Docker images"},{"location":"user-docs/install/#helm-recommended","text":"The recommended way to install an ACK service controller for Kubernetes is to use Helm 3. Please ensure you have installed Helm 3 to your local environment before running these steps. Each ACK service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the ACK service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. To view the Helm charts available for installation, check the ECR public repository for the ACK Helm charts . Click on the \"Image tags\" tab and take a note of the Helm chart tag for the service controller and version you wish to install. Before installing a Helm chart, you must first make the Helm chart available on the deployment host. To do so, use the helm chart pull and helm chart export commands: 1 2 3 4 5 6 7 8 9 10 11 export HELM_EXPERIMENTAL_OCI = 1 export SERVICE = s3 export RELEASE_VERSION = v0.0.1 export CHART_EXPORT_PATH = /tmp/chart export CHART_REPO = public.ecr.aws/aws-controllers-k8s/chart export CHART_REF = $CHART_REPO : $SERVICE - $RELEASE_VERSION mkdir -p $CHART_EXPORT_PATH helm chart pull $CHART_REF helm chart export $CHART_REF --destination $CHART_EXPORT_PATH You then install a particular ACK service controller using the helm install CLI command: 1 2 3 4 5 6 export ACK_K8S_NAMESPACE = ack-system kubectl create namespace $ACK_K8S_NAMESPACE helm install --namespace $ACK_K8S_NAMESPACE ack- $SERVICE -controller \\ $CHART_EXPORT_PATH /ack- $SERVICE -controller You will see the Helm chart installed: 1 2 3 4 5 6 7 $ helm install --namespace $ACK_K8S_NAMESPACE ack- $SERVICE -controller $CHART_EXPORT_PATH /ack- $SERVICE -controller NAME: ack-s3-controller LAST DEPLOYED: Thu Dec 17 13 :09:17 2020 NAMESPACE: ack-system STATUS: deployed REVISION: 1 TEST SUITE: None You may then verify the Helm chart was installed using the helm list command: 1 helm list --namespace $ACK_K8S_NAMESPACE -o yaml you should see your newly-deployed Helm chart release: 1 2 3 4 5 6 7 8 $ helm list --namespace $ACK_K8S_NAMESPACE -o yaml - app_version: v0.0.1 chart: ack-s3-controller-v0.0.1 name: ack-s3-controller namespace: ack-system revision: \"1\" status: deployed updated: 2020 -12-17 13 :09:17.309002201 -0500 EST","title":"Helm (recommended)"},{"location":"user-docs/install/#static-kubernetes-manifests","text":"If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. TODO(jaypipes)","title":"Static Kubernetes manifests"},{"location":"user-docs/irsa/","text":"Setting up ACK with IAM Roles for Service Accounts \u00b6 IAM Roles for Service Accounts , or IRSA, is a system that automates the provisioning and rotation of IAM temporary credentials (called a Web Identity) that a Kubernetes ServiceAccount can use to call AWS APIs. The primary advantage of IRSA is that Kubernetes Pods which use the ServiceAccount associated with an IAM Role can have a reduced IAM permission footprint than the IAM Role in use for the Kubernetes EC2 worker node (known as the EC2 Instance Profile Role). This security concept is known as Least Privilege . For example, assume you have a broadly-scoped IAM Role with permissions to access the Instance Metadata Service (IMDS) from the EC2 worker node. If you do not want Kubernetes Pods running on that EC2 Instance to have access to IMDS, you can create a different IAM Role with a reduced permission set and associate this reduced-scope IAM Role with the Kubernetes ServiceAccount the Pod uses. IRSA will ensure that a special file is injected (and rotated periodically) into the Pod that contains a JSON Web Token (JWT) that encapsulates a request for temporary credentials to assume the IAM Role with reduced permissions. When AWS clients or SDKs connect to an AWS API, they detect the existence of this special token file and call the STS::AssumeRoleWithWebIdentity API to assume the IAM Role with reduced permissions. EKS is not required to use IRSA Note that you do not need to be using the Amazon EKS service in order to use IRSA. There are instructions on the amazon-eks-pod-identity-webhook repository for setting up IRSA on your own Kubernetes installation. IRSA setup on EKS cluster and install ACK controller using Helm \u00b6 Following steps provide example to setup IRSA on EKS cluster to install ACK ElastiCache controller using Helm charts. By modifying the variables values as needed, these steps can be applied for other ACK controllers. The steps include: 1. Create OIDC identity provider for cluster \u00b6 Create OIDC identity provider for cluster using CLI command. Example: 1 2 EKS_CLUSTER_NAME=<eks cluster name> eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --approve For detailed instructions, follow Enabling IAM roles for service accounts on your cluster . 2. Create an IAM role and policy for service account \u00b6 For detailed instructions, follow instructions at Creating an IAM role and policy for your service account . 2(a) - Create IAM role \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 AWS_ACCOUNT_ID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) OIDC_PROVIDER =$ ( aws eks describe - cluster -- name $ EKS_CLUSTER_NAME -- query \"cluster.identity.oidc.issuer\" -- output text | sed - e \"s/^https:\\/\\///\" ) ACK_K8S_NAMESPACE = ack - system ACK_K8S_SERVICE_ACCOUNT_NAME = ack - elasticache - controller read - r - d '' TRUST_RELATIONSHIP << EOF { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"${OIDC_PROVIDER}:sub\" : \"system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_K8S_SERVICE_ACCOUNT_NAME}\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" > trust . json # update variables as needed AWS_SERVICE_NAME = 'elasticache' ACK_CONTROLLER_IAM_ROLE = \"ack-${AWS_SERVICE_NAME}-controller\" ACK_CONTROLLER_IAM_ROLE_DESCRIPTION = 'IRSA role for ACK $AWS_SERVICE_NAME controller deployment on EKS cluster using Helm charts' aws iam create - role -- role - name \"${ACK_CONTROLLER_IAM_ROLE}\" -- assume - role - policy - document file : // trust . json -- description \"${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}\" 2(b) - Attach IAM policy to role \u00b6 1 2 3 4 5 6 # This example uses pre-existing policy for ElastiCache # Create an IAM policy and use its ARN and update IAM_POLICY_ARN variable as needed IAM_POLICY_ARN = 'arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess' aws iam attach - role - policy \\ -- role - name \"${ACK_CONTROLLER_IAM_ROLE}\" \\ -- policy - arn \"$IAM_POLICY_ARN\" 3. Associate an IAM role to service account \u00b6 For detailed instructions, follow instructions at Associate an IAM role to a service account . 3(a) - If Helm charts available on local file system \u00b6 Update values.yaml and set value for aws.region , serviceAccount.annotations . 1 2 3 4 5 6 7 8 9 10 11 12 13 # update variables as needed ACK_CONTROLLER_HELM_CHARTS_DIR =< directory containing Helm chart for ACK service controller > AWS_SERVICE_NAME = 'elasticache' ACK_K8S_NAMESPACE = ack - system ACK_K8S_RELEASE_NAME = ack -$ AWS_SERVICE_NAME - controller kubectl create namespace \"$ACK_K8S_NAMESPACE\" cd \"$ACK_CONTROLLER_HELM_CHARTS_DIR\" # dry run and view the resultant output helm install -- debug -- dry - run -- namespace \"$ACK_K8S_NAMESPACE\" \"$ACK_K8S_RELEASE_NAME\" . # install on cluster helm install -- namespace \"$ACK_K8S_NAMESPACE\" \"$ACK_K8S_RELEASE_NAME\" . Verify that the service account has been created on cluster and that its annotation include IAM Role (created during Step#2 above) arn: 1 kubectl describe serviceaccount/$ACK_K8S_SERVICE_ACCOUNT_NAME -n $ACK_K8S_NAMESPACE 3(b) - If Helm charts have already been installed on cluster without modifying values.yaml \u00b6 For example, if installation was done as: 1 2 3 4 AWS_SERVICE_NAME='elasticache' ACK_K8S_NAMESPACE=ack-system ACK_K8S_RELEASE_NAME=ack-$AWS_SERVICE_NAME-controller helm install --namespace $ACK_K8S_NAMESPACE ack-$AWS_SERVICE_NAME-controller $ACK_K8S_RELEASE_NAME Then service account would already exist on the cluster; however its association with IAM Role would be pending. Verify it using: 1 kubectl describe serviceaccount/$ACK_K8S_SERVICE_ACCOUNT_NAME -n $ACK_K8S_NAMESPACE Observe that the arn of IAM Role (created during Step#2 above) is not set as annotation for the service account. To associate an IAM role to service account: 1 2 3 # annotate service account with service role arn. ISRA_ROLE_ARN=<role arn> kubectl annotate serviceaccount -n $ACK_K8S_NAMESPACE $ACK_K8S_SERVICE_ACCOUNT_NAME $ISRA_ROLE_ARN Update aws region to use in the controller, if not done already: 1 2 3 4 # update desired AWS region . example: us - east - 1 AWS_REGION =< aws region id > kubectl - n $ACK_K8S_NAMESPACE set env deployment / $ACK_K8S_RELEASE_NAME \\ AWS_REGION = \"$AWS_ACCOUNT_ID\" Verify \u00b6 Describe one of the pods and verify that the AWS_WEB_IDENTITY_TOKEN_FILE and AWS_ROLE_ARN environment variables exist. 1 2 kubectl get pods - A kubectl exec - n kube - system aws - node -< 9 rgzw > env | grep AWS verify the output, example: 1 2 3 AWS_VPC_K8S_CNI_LOGLEVEL = DEBUG AWS_ROLE_ARN = arn : aws : iam :: < AWS_ACCOUNT_ID > : role /< IAM_ROLE_NAME > AWS_WEB_IDENTITY_TOKEN_FILE =/ var / run / secrets / eks . amazonaws . com / serviceaccount / token \u00b6","title":"Setting up ACK with IAM Roles for Service Accounts"},{"location":"user-docs/irsa/#setting-up-ack-with-iam-roles-for-service-accounts","text":"IAM Roles for Service Accounts , or IRSA, is a system that automates the provisioning and rotation of IAM temporary credentials (called a Web Identity) that a Kubernetes ServiceAccount can use to call AWS APIs. The primary advantage of IRSA is that Kubernetes Pods which use the ServiceAccount associated with an IAM Role can have a reduced IAM permission footprint than the IAM Role in use for the Kubernetes EC2 worker node (known as the EC2 Instance Profile Role). This security concept is known as Least Privilege . For example, assume you have a broadly-scoped IAM Role with permissions to access the Instance Metadata Service (IMDS) from the EC2 worker node. If you do not want Kubernetes Pods running on that EC2 Instance to have access to IMDS, you can create a different IAM Role with a reduced permission set and associate this reduced-scope IAM Role with the Kubernetes ServiceAccount the Pod uses. IRSA will ensure that a special file is injected (and rotated periodically) into the Pod that contains a JSON Web Token (JWT) that encapsulates a request for temporary credentials to assume the IAM Role with reduced permissions. When AWS clients or SDKs connect to an AWS API, they detect the existence of this special token file and call the STS::AssumeRoleWithWebIdentity API to assume the IAM Role with reduced permissions. EKS is not required to use IRSA Note that you do not need to be using the Amazon EKS service in order to use IRSA. There are instructions on the amazon-eks-pod-identity-webhook repository for setting up IRSA on your own Kubernetes installation.","title":"Setting up ACK with IAM Roles for Service Accounts"},{"location":"user-docs/irsa/#irsa-setup-on-eks-cluster-and-install-ack-controller-using-helm","text":"Following steps provide example to setup IRSA on EKS cluster to install ACK ElastiCache controller using Helm charts. By modifying the variables values as needed, these steps can be applied for other ACK controllers. The steps include:","title":"IRSA setup on EKS cluster and install ACK controller using Helm"},{"location":"user-docs/irsa/#1-create-oidc-identity-provider-for-cluster","text":"Create OIDC identity provider for cluster using CLI command. Example: 1 2 EKS_CLUSTER_NAME=<eks cluster name> eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --approve For detailed instructions, follow Enabling IAM roles for service accounts on your cluster .","title":"1. Create OIDC identity provider for cluster"},{"location":"user-docs/irsa/#2-create-an-iam-role-and-policy-for-service-account","text":"For detailed instructions, follow instructions at Creating an IAM role and policy for your service account .","title":"2. Create an IAM role and policy for service account"},{"location":"user-docs/irsa/#2a-create-iam-role","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 AWS_ACCOUNT_ID =$ ( aws sts get - caller - identity -- query \"Account\" -- output text ) OIDC_PROVIDER =$ ( aws eks describe - cluster -- name $ EKS_CLUSTER_NAME -- query \"cluster.identity.oidc.issuer\" -- output text | sed - e \"s/^https:\\/\\///\" ) ACK_K8S_NAMESPACE = ack - system ACK_K8S_SERVICE_ACCOUNT_NAME = ack - elasticache - controller read - r - d '' TRUST_RELATIONSHIP << EOF { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"${OIDC_PROVIDER}:sub\" : \"system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_K8S_SERVICE_ACCOUNT_NAME}\" } } } ] } EOF echo \"${TRUST_RELATIONSHIP}\" > trust . json # update variables as needed AWS_SERVICE_NAME = 'elasticache' ACK_CONTROLLER_IAM_ROLE = \"ack-${AWS_SERVICE_NAME}-controller\" ACK_CONTROLLER_IAM_ROLE_DESCRIPTION = 'IRSA role for ACK $AWS_SERVICE_NAME controller deployment on EKS cluster using Helm charts' aws iam create - role -- role - name \"${ACK_CONTROLLER_IAM_ROLE}\" -- assume - role - policy - document file : // trust . json -- description \"${ACK_CONTROLLER_IAM_ROLE_DESCRIPTION}\"","title":"2(a) - Create IAM role"},{"location":"user-docs/irsa/#2b-attach-iam-policy-to-role","text":"1 2 3 4 5 6 # This example uses pre-existing policy for ElastiCache # Create an IAM policy and use its ARN and update IAM_POLICY_ARN variable as needed IAM_POLICY_ARN = 'arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess' aws iam attach - role - policy \\ -- role - name \"${ACK_CONTROLLER_IAM_ROLE}\" \\ -- policy - arn \"$IAM_POLICY_ARN\"","title":"2(b) - Attach IAM policy to role"},{"location":"user-docs/irsa/#3-associate-an-iam-role-to-service-account","text":"For detailed instructions, follow instructions at Associate an IAM role to a service account .","title":"3. Associate an IAM role to service account"},{"location":"user-docs/irsa/#3a-if-helm-charts-available-on-local-file-system","text":"Update values.yaml and set value for aws.region , serviceAccount.annotations . 1 2 3 4 5 6 7 8 9 10 11 12 13 # update variables as needed ACK_CONTROLLER_HELM_CHARTS_DIR =< directory containing Helm chart for ACK service controller > AWS_SERVICE_NAME = 'elasticache' ACK_K8S_NAMESPACE = ack - system ACK_K8S_RELEASE_NAME = ack -$ AWS_SERVICE_NAME - controller kubectl create namespace \"$ACK_K8S_NAMESPACE\" cd \"$ACK_CONTROLLER_HELM_CHARTS_DIR\" # dry run and view the resultant output helm install -- debug -- dry - run -- namespace \"$ACK_K8S_NAMESPACE\" \"$ACK_K8S_RELEASE_NAME\" . # install on cluster helm install -- namespace \"$ACK_K8S_NAMESPACE\" \"$ACK_K8S_RELEASE_NAME\" . Verify that the service account has been created on cluster and that its annotation include IAM Role (created during Step#2 above) arn: 1 kubectl describe serviceaccount/$ACK_K8S_SERVICE_ACCOUNT_NAME -n $ACK_K8S_NAMESPACE","title":"3(a) - If Helm charts available on local file system"},{"location":"user-docs/irsa/#3b-if-helm-charts-have-already-been-installed-on-cluster-without-modifying-valuesyaml","text":"For example, if installation was done as: 1 2 3 4 AWS_SERVICE_NAME='elasticache' ACK_K8S_NAMESPACE=ack-system ACK_K8S_RELEASE_NAME=ack-$AWS_SERVICE_NAME-controller helm install --namespace $ACK_K8S_NAMESPACE ack-$AWS_SERVICE_NAME-controller $ACK_K8S_RELEASE_NAME Then service account would already exist on the cluster; however its association with IAM Role would be pending. Verify it using: 1 kubectl describe serviceaccount/$ACK_K8S_SERVICE_ACCOUNT_NAME -n $ACK_K8S_NAMESPACE Observe that the arn of IAM Role (created during Step#2 above) is not set as annotation for the service account. To associate an IAM role to service account: 1 2 3 # annotate service account with service role arn. ISRA_ROLE_ARN=<role arn> kubectl annotate serviceaccount -n $ACK_K8S_NAMESPACE $ACK_K8S_SERVICE_ACCOUNT_NAME $ISRA_ROLE_ARN Update aws region to use in the controller, if not done already: 1 2 3 4 # update desired AWS region . example: us - east - 1 AWS_REGION =< aws region id > kubectl - n $ACK_K8S_NAMESPACE set env deployment / $ACK_K8S_RELEASE_NAME \\ AWS_REGION = \"$AWS_ACCOUNT_ID\"","title":"3(b) - If Helm charts have already been installed on cluster without modifying values.yaml"},{"location":"user-docs/irsa/#verify","text":"Describe one of the pods and verify that the AWS_WEB_IDENTITY_TOKEN_FILE and AWS_ROLE_ARN environment variables exist. 1 2 kubectl get pods - A kubectl exec - n kube - system aws - node -< 9 rgzw > env | grep AWS verify the output, example: 1 2 3 AWS_VPC_K8S_CNI_LOGLEVEL = DEBUG AWS_ROLE_ARN = arn : aws : iam :: < AWS_ACCOUNT_ID > : role /< IAM_ROLE_NAME > AWS_WEB_IDENTITY_TOKEN_FILE =/ var / run / secrets / eks . amazonaws . com / serviceaccount / token","title":"Verify"},{"location":"user-docs/irsa/#_1","text":"","title":""},{"location":"user-docs/usage/","text":"Usage \u00b6 In this section we discuss how to use AWS Controllers for Kubernetes. Prerequisites \u00b6 Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles. Creating an AWS resource via the Kubernetes API \u00b6 TODO Viewing AWS resource information via the Kubernetes API \u00b6 TODO Deleting an AWS resource via the Kubernetes API \u00b6 TODO Modifying an AWS resource via the Kubernetes API \u00b6 TODO","title":"Usage"},{"location":"user-docs/usage/#usage","text":"In this section we discuss how to use AWS Controllers for Kubernetes.","title":"Usage"},{"location":"user-docs/usage/#prerequisites","text":"Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles.","title":"Prerequisites"},{"location":"user-docs/usage/#creating-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Creating an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#viewing-aws-resource-information-via-the-kubernetes-api","text":"TODO","title":"Viewing AWS resource information via the Kubernetes API"},{"location":"user-docs/usage/#deleting-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Deleting an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#modifying-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Modifying an AWS resource via the Kubernetes API"},{"location":"user-docs/wip/","text":"Work in progress ... \u00b6 ... check back later for end-user facing install and usage of ACK. Tip Use the developer preview and let us know via the issue tracker if something doesn't work as described there.","title":"Usage"},{"location":"user-docs/wip/#work-in-progress","text":"... check back later for end-user facing install and usage of ACK. Tip Use the developer preview and let us know via the issue tracker if something doesn't work as described there.","title":"Work in progress ..."}]}